{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de dossiers pour enregistrer les fichiers\n",
    "\n",
    "Ce code initialise les dossiers nécessaires pour stocker les fichiers d’images et de documents PDF. Si les dossiers n'existent pas encore dans le répertoire de travail, ils sont créés automatiquement.\n",
    "\n",
    "``image_folder = 'Images'`` :\n",
    "\n",
    "- Définition du nom du dossier où toutes les images récupérées ou téléchargées seront stockées.\n",
    "- ``image_folder`` servira de variable pour gérer le chemin d'accès aux images dans le reste du code.\n",
    "\n",
    "\n",
    "``pdf_folder = 'PDFs'`` :\n",
    "\n",
    "- Définition du nom du dossier où tous les fichiers PDF récupérés ou téléchargés seront stockés.\n",
    "- p``df_folder`` est utilisé comme référence pour le chemin des fichiers PDF.\n",
    "\n",
    "``os.makedirs(image_folder, exist_ok=True)`` :\n",
    "\n",
    "- Utilise ``os.makedirs`` pour créer le dossier spécifié dans ``image_folder``.\n",
    "- Le paramètre ``exist_ok=True`` vérifie si le dossier existe déjà ; si c’est le cas, il ne le recrée pas, évitant ainsi une erreur.\n",
    "\n",
    "``os.makedirs(pdf_folder, exist_ok=True)`` :\n",
    "\n",
    "Même fonction que pour image_folder, mais ici pour le dossier pdf_folder pour stocker les fichiers PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'Images'\n",
    "pdf_folder = 'PDFs'\n",
    "\n",
    "os.makedirs(image_folder, exist_ok=True)  # Créer le dossier Images s'il n'existe pas\n",
    "os.makedirs(pdf_folder, exist_ok=True)    # Créer le dossier PDFs s'il n'existe pas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion à MongoDB et nettoyage de texte\n",
    "\n",
    "Connexion MongoDB :\n",
    "\n",
    "- MongoClient : Crée une connexion au serveur MongoDB en utilisant une URL MongoDB.\n",
    "- client[``'nom_de_ta_base'``] : Accède à une base de données spécifique, ici indiquée par 'nom_de_ta_base' (à remplacer par le nom réel de la base).\n",
    "- Cette connexion permet d’interagir avec MongoDB pour stocker, récupérer et manipuler des données.\n",
    "\n",
    "Fonction ``clean_text`` :\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``text`` : Chaîne de caractères à nettoyer, qui peut inclure des balises HTML ou des caractères spéciaux.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie le texte nettoyé sous forme de chaîne de caractères, sans balises HTML, avec des sauts de ligne appropriés pour une meilleure lisibilité.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Remplace les espaces insécables ``(\\xa0)`` par des espaces standard pour uniformiser le texte.\n",
    "- Ajoute des sauts de ligne après chaque balise `<li>` pour que les éléments de liste apparaissent sur des lignes séparées dans le texte extrait.\n",
    "- Utilise BeautifulSoup pour analyser et supprimer toutes les balises HTML restantes.\n",
    "- Extrait uniquement le texte brut et le retourne, avec une mise en forme propre et lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à la base de données MongoDB\n",
    "client = MongoClient('mongodb+srv://serginemengue46:tu3uF7Ap0g2RQDou@cluster0.7xuvx.mongodb.net')\n",
    "db = client['nom_de_ta_base']  # Remplace par le nom de ta base de données\n",
    "# Création de la connexion à MongoDB en spécifiant l'URL du serveur MongoDB. \n",
    "# Cette connexion permet d'accéder à une base de données spécifique en indiquant son nom.\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    \n",
    "    text = text.replace('\\xa0', ' ').strip()\n",
    "    # Remplace les caractères d'espace insécable ('\\xa0') par des espaces standard et supprime les espaces superflus.\n",
    "\n",
    "    text = text.replace('<li>', '<li>\\n')\n",
    "    # Ajoute un saut de ligne après chaque balise <li> pour rendre les éléments de liste plus lisibles dans le texte brut.\n",
    "\n",
    "    soup = bs(text, 'html.parser')\n",
    "    # Utilise BeautifulSoup pour analyser le texte HTML et supprimer toutes les balises HTML restantes.\n",
    "\n",
    "    cleaned_text = soup.get_text(separator='\\n').strip()\n",
    "    # Extrait le texte brut avec des sauts de ligne entre les éléments et supprime les espaces superflus en début et fin de texte.\n",
    "\n",
    "    return cleaned_text\n",
    "    # Retourne le texte nettoyé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insertion des données dans la bdd MongoDB\n",
    "\n",
    "La fonction ``insert_to_db`` gère l’insertion des données dans MongoDB en les structurant par site et par thème. Elle permet de stocker les données de manière organisée en créant des collections spécifiques à chaque site.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "``site_name`` : Chaîne de caractères représentant le nom du site, qui est utilisé comme nom de collection dans MongoDB.\n",
    "``theme_name`` : Chaîne de caractères représentant le nom du thème, pour différencier les données selon le sujet.\n",
    "``data`` : Liste ou dictionnaire contenant les données à insérer pour le thème, représentant le contenu extrait.\n",
    "\n",
    "Retour : Aucun retour direct. La fonction insère les données dans MongoDB et affiche un message de confirmation.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Sélectionne une collection dans MongoDB en fonction de ``site_name``. Si la collection n'existe pas, MongoDB la crée automatiquement.\n",
    "- Crée un document ``theme_data`` contenant le nom du site, le thème, et les données associées.\n",
    "- Insère ``theme_data`` dans la collection spécifiée. En cas d’insertion réussie, un message de confirmation est affiché avec le nom du thème et de la collection.\n",
    "- Gère les erreurs avec un bloc ``try-except`` pour afficher un message explicatif en cas de problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_db(site_name, theme_name, data):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Sélection de la collection par nom de site\n",
    "        collection = db[site_name]  # Collection nommée d'après le site pour organiser les données\n",
    "        # Cette ligne permet de sélectionner ou de créer une collection dans MongoDB en fonction du nom du site.\n",
    "\n",
    "        # Créer un document séparé pour chaque thème\n",
    "        theme_data = {\n",
    "            'site_name': site_name,\n",
    "            'theme_name': theme_name,\n",
    "            'data': data\n",
    "        }\n",
    "        # Structure le document MongoDB avec le nom du site, le thème et les données extraites.\n",
    "\n",
    "        # Insérer le document pour le thème dans MongoDB\n",
    "        collection.insert_one(theme_data)\n",
    "        print(f\"Données pour le thème '{theme_name}' insérées avec succès dans la collection '{site_name}'.\")\n",
    "        # Ajoute le document à la collection MongoDB et confirme l'insertion avec un message.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de l'insertion dans MongoDB : {str(e)}\")\n",
    "        # Capture et affiche les erreurs éventuelles survenues lors de l'insertion des données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Télechargement d'images\n",
    "\n",
    "La fonction ``download_images`` télécharge toutes les images disponibles dans une page web et les enregistre localement dans un dossier défini pour les images. Elle utilise BeautifulSoup pour trouver les balises d'image et requests pour récupérer le contenu des images.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``soup`` : Un objet BeautifulSoup représentant la page web analysée.\n",
    "- ``link`` : L'URL de la page d'où proviennent les images, utilisée pour générer des noms uniques pour les fichiers.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie une liste contenant les chemins des images téléchargées. Si aucune image n'est téléchargée ou en cas d'erreur, retourne ['N/A'].\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Récupère toutes les balises ``<img``> avec un attribut src pour collecter les URLs des images.\n",
    "- Pour chaque URL d'image, télécharge l'image en utilisant requests.get.\n",
    "- Génére un nom unique pour chaque image en utilisant une partie de l'URL de la page et un index.\n",
    "- Sauvegarde chaque image dans le dossier défini, puis ajoute le chemin du fichier téléchargé à une liste.\n",
    "- Retourne la liste des chemins des images téléchargées ou ``['N/A']`` en cas d'échec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(soup, link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        images = soup.find_all('img')\n",
    "        image_urls = [img['src'] for img in images if 'src' in img.attrs]\n",
    "        # Récupère toutes les balises <img> avec un attribut 'src' et compile leurs URLs.\n",
    "\n",
    "        downloaded_images = []\n",
    "        # Initialise une liste pour stocker les chemins des images téléchargées.\n",
    "\n",
    "        for i, img_url in enumerate(image_urls):\n",
    "            img_data = requests.get(img_url).content\n",
    "            # Télécharge le contenu de l'image depuis son URL.\n",
    "\n",
    "            img_name = f\"{link.split('/')[-1]}_{i}.jpg\"\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            # Définit un nom unique pour chaque image en utilisant une partie de l'URL de la page source et un index.\n",
    "            \n",
    "            with open(img_path, 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "                # Écrit le contenu de l'image dans un fichier local avec accès en mode écriture binaire.\n",
    "            \n",
    "            downloaded_images.append(img_path)\n",
    "            # Ajoute le chemin de l'image téléchargée à la liste.\n",
    "\n",
    "        return downloaded_images if downloaded_images else ['N/A']\n",
    "        # Retourne la liste des images téléchargées, ou 'N/A' si aucune image n'a été téléchargée.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors du téléchargement des images : {str(e)}\")\n",
    "        return ['N/A']\n",
    "        # En cas d'erreur, affiche un message d'erreur et retourne 'N/A'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Télechargement de pdfs\n",
    "\n",
    "La fonction ``download_pdfs`` télécharge tous les fichiers PDF disponibles dans une page web et les enregistre localement dans un dossier spécifié pour les PDF. Elle utilise BeautifulSoup pour trouver les balises qui contiennent des liens vers les PDF et requests pour récupérer le contenu des fichiers.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``soup`` : Un objet BeautifulSoup représentant la page web analysée.\n",
    "- ``link`` : L'URL de la page d'où proviennent les fichiers PDF, utilisée pour générer des noms uniques pour les fichiers.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie une liste contenant les chemins des PDF téléchargés. Si aucun PDF n'est téléchargé ou en cas d'erreur, retourne ``['N/A']``.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Identifie les balises`` <div>`` spécifiques qui contiennent les liens vers les PDF en fonction de leurs classes CSS.\n",
    "- Récupère les URLs des PDF dont le lien est marqué avec le titre 'Télécharger'.\n",
    "- Pour chaque URL de PDF, télécharge le fichier en utilisant requests.get.\n",
    "- Génère un nom unique pour chaque PDF en utilisant une partie de l'URL de la page source et un index.\n",
    "- Sauvegarde chaque PDF dans le dossier défini, puis ajoute le chemin du fichier téléchargé à une liste.\n",
    "- Retourne la liste des chemins des PDF téléchargés ou ``['N/A']`` en cas d'échec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(soup, link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        pdf_divs = soup.find_all('div', class_=[\n",
    "            'col-sm-12 col-xl-5- col-xxl-4 action-hover- d-flex flex-column justify-center', \n",
    "            'col-sm-3 col-md-6- col-lg-4 col-xl-3 col-xxl-2 action-hover- d-flex flex-column justify-center align-items-center'\n",
    "        ])\n",
    "        # Recherche tous les <div> qui contiennent des liens vers des PDF, en utilisant leurs classes spécifiques.\n",
    "\n",
    "        pdf_urls = [\n",
    "            div.find('a', href=True)['href'] for div in pdf_divs \n",
    "            if div.find('a', href=True) and div.find('a', href=True).get('title') == 'Télécharger'\n",
    "        ]\n",
    "        # Récupère les URLs des fichiers PDF en filtrant les liens dont le titre est 'Télécharger'.\n",
    "\n",
    "        downloaded_pdfs = []\n",
    "        # Initialise une liste pour stocker les chemins des PDF téléchargés.\n",
    "\n",
    "        for i, pdf_url in enumerate(pdf_urls):\n",
    "            pdf_data = requests.get(pdf_url).content\n",
    "            # Télécharge le contenu du fichier PDF depuis son URL.\n",
    "\n",
    "            pdf_name = f\"{link.split('/')[-1]}_{i}.pdf\"\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_name)\n",
    "            # Définit un nom unique pour chaque PDF en utilisant une partie de l'URL de la page source et un index.\n",
    "            \n",
    "            with open(pdf_path, 'wb') as handler:\n",
    "                handler.write(pdf_data)\n",
    "                # Écrit le contenu du PDF dans un fichier local avec accès en mode écriture binaire.\n",
    "            \n",
    "            downloaded_pdfs.append(pdf_path)\n",
    "            # Ajoute le chemin du PDF téléchargé à la liste.\n",
    "\n",
    "        return downloaded_pdfs if downloaded_pdfs else ['N/A']\n",
    "        # Retourne la liste des PDF téléchargés, ou 'N/A' si aucun PDF n'a été téléchargé.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors du téléchargement des PDFs : {str(e)}\")\n",
    "        return ['N/A']\n",
    "        # En cas d'erreur, affiche un message d'erreur et retourne 'N/A'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extration des informations sur une page web\n",
    "\n",
    "La fonction ``get_info extrait`` les informations principales d’une page web, telles que la catégorie, le thème, la préface, la date, le titre, le texte, les liens utiles et les ressources (images et PDF) associées, puis les retourne dans un dictionnaire.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``link`` : URL de la page à analyser et à partir de laquelle extraire les informations.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie un dictionnaire contenant les informations extraites. En cas d’erreur, retourne un dictionnaire avec 'N/A' pour chaque champ.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Utilise BeautifulSoup pour récupérer le contenu de la page et analyser les balises HTML spécifiques.\n",
    "- Extrait les informations texte des éléments de catégorie, thème, préface, date, titre, et texte.\n",
    "- Récupère les liens utiles spécifiés, concaténant leurs URLs en une seule chaîne.\n",
    "- Télécharge les images et PDF associés en utilisant des fonctions dédiées et ajoute leurs chemins au dictionnaire.\n",
    "- Retourne un dictionnaire contenant toutes les informations extraites, ou un dictionnaire de valeurs par défaut en cas d'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        page = requests.get(link)\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        # Envoie une requête pour obtenir le contenu de la page, puis l'analyse avec BeautifulSoup.\n",
    "\n",
    "        # Extraire la catégorie de l'article\n",
    "        categorie_element = soup.find('h1', class_='h2')\n",
    "        categorie = categorie_element.text.strip() if categorie_element else 'N/A'\n",
    "        # Récupère et nettoie le texte de la catégorie, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire le thème de l'article\n",
    "        theme_element = soup.find('h1', class_='wiki-title-theme')\n",
    "        theme = theme_element.text.strip() if theme_element else 'N/A'\n",
    "        # Récupère et nettoie le texte du thème, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire la préface de l'article\n",
    "        preface_element = soup.find('p', class_='lead')\n",
    "        preface = preface_element.text.strip() if preface_element else 'N/A'\n",
    "        # Récupère et nettoie le texte de la préface, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire la date de publication\n",
    "        date_element = soup.find('small', class_='app-card-date text-muted')\n",
    "        date = date_element.text.strip() if date_element else 'N/A'\n",
    "        # Récupère et nettoie le texte de la date, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire le titre de l'article\n",
    "        titre_elements = soup.find_all('h2', class_=['wiki-title-def', 'app-post-title'])\n",
    "        titre = ' / '.join([elem.text.strip() for elem in titre_elements]) if titre_elements else 'N/A'\n",
    "        # Récupère et concatène tous les titres trouvés dans les balises spécifiées, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire le contenu texte principal\n",
    "        texte_elements = soup.find_all(['div', 'article'], class_=['card-text', 'kPost-content'])\n",
    "        texte = ' '.join([elem.get_text(separator='\\n').strip() if elem else 'N/A' for elem in texte_elements]) if texte_elements else 'N/A'\n",
    "        # Récupère et concatène le contenu texte des sections principales, ou retourne 'N/A' si non trouvé.\n",
    "\n",
    "        # Extraire les liens utiles\n",
    "        liens_utiles = []\n",
    "        for a_tag in soup.find_all('a', href=True, title=True):\n",
    "            if a_tag['title'].strip() in [\"Je me lance\", \"Accéder à l'outil\"]:\n",
    "                liens_utiles.append(a_tag['href'])\n",
    "        liens_utiles = ', '.join(liens_utiles) if liens_utiles else 'N/A'\n",
    "        # Récupère les liens ayant des titres spécifiques et les concatène en une chaîne séparée par des virgules.\n",
    "\n",
    "        # Télécharger les images et les PDFs associés à la page\n",
    "        downloaded_images = download_images(soup, link)\n",
    "        downloaded_pdfs = download_pdfs(soup, link)\n",
    "        # Utilise des fonctions de téléchargement d'images et de PDFs.\n",
    "\n",
    "        # Créer un dictionnaire avec toutes les informations extraites\n",
    "        document = {\n",
    "            'Catégorie': categorie,\n",
    "            'Date': date,\n",
    "            'Thème': theme,\n",
    "            'Préface': preface,\n",
    "            'Titre': titre,\n",
    "            'Texte': texte,\n",
    "            'Lien': link,\n",
    "            'Liens utiles': liens_utiles,\n",
    "            'Images': downloaded_images,\n",
    "            'PDFs': downloaded_pdfs\n",
    "        }\n",
    "\n",
    "        return document\n",
    "        # Retourne le dictionnaire avec les données extraites.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de l'extraction des informations : {str(e)}\")\n",
    "        # Affiche un message d'erreur en cas de problème d'extraction.\n",
    "        \n",
    "        # Retourne un dictionnaire avec 'N/A' pour chaque champ si une erreur se produit.\n",
    "        return {\n",
    "            'Catégorie': 'N/A',\n",
    "            'Date': 'N/A',\n",
    "            'Thème': 'N/A',\n",
    "            'Préface': 'N/A',\n",
    "            'Titre': 'N/A',\n",
    "            'Texte': 'N/A',\n",
    "            'Lien': link,\n",
    "            'Liens utiles': 'N/A',\n",
    "            'Images': 'N/A',\n",
    "            'PDFs': 'N/A'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des liens d'une page web, récupération des informations associées aux liens, et insertion dans MongoDB\n",
    "\n",
    "Ce code extrait des liens d'une page web, récupère les informations associées aux liens, et les insère dans MongoDB en les structurant par thèmes.\n",
    "\n",
    "Fonctions et paramètres :\n",
    "\n",
    "- ``extract_links(section_id)``: Extrait les liens dans une section de la page HTML identifiée par section_id.\n",
    "- `section_id`: Chaîne représentant l'ID de la section dans la page (par exemple, ``'collapse-bas-carbone'``).\n",
    "- ``site_name``: Nom du site, utilisé pour générer l'URL complète de la page.\n",
    "\n",
    "Retour des fonctions :\n",
    "\n",
    "- ``extract_links`` retourne une liste de liens extraits de la section. Si aucun lien n'est trouvé, elle retourne une liste vide.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Envoie une requête pour charger le contenu de la page spécifiée par url.\n",
    "- Utilise ``extract_links`` pour récupérer les liens des sections de thèmes comme ``\"Bas carbone\"`` et ``\"BIM\"``.\n",
    "- Pour chaque lien, appelle ``get_info`` pour extraire des informations spécifiques sur chaque page liée, et stocke les résultats dans des listes (``bas_carbone_data`` pour ``\"Bas carbone\"`` et ``bim_data`` pour ``\"BIM\"``).\n",
    "- Utilise ``insert_to_db`` pour insérer les informations dans MongoDB, organisées par thème et nom de site.\n",
    "\n",
    "Utilisation :\n",
    "\n",
    "- Le code principal effectue la requête initiale, l'extraction des liens, et le traitement pour chaque lien extrait, puis organise et insère les données dans MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(section_id):\n",
    "    \n",
    "    section = soup.find('div', id=section_id)\n",
    "    links = []\n",
    "    # Initialise une liste vide pour stocker les liens trouvés dans la section.\n",
    "\n",
    "    if section:\n",
    "        list_items = section.find_all('li', class_='list-item col-12 col-lg-6 py-2')\n",
    "        for item in list_items:\n",
    "            link = item.find('a', href=True)\n",
    "            if link:\n",
    "                links.append(link['href'])\n",
    "        # Si la section est trouvée, recherche toutes les balises <li> contenant des liens <a> et les ajoute à la liste.\n",
    "\n",
    "    return links\n",
    "    # Retourne la liste des liens extraits ou une liste vide si aucun lien n'est trouvé.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Nom du site à utiliser pour construire l'URL\n",
    "    site_name = \"lab.cercle-promodul\"\n",
    "    url = f\"https://{site_name}.inef4.org/themes\"\n",
    "    # Définit l'URL de la page web à analyser en fonction du nom du site.\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    # Envoie une requête pour obtenir le contenu de la page et l'analyse avec BeautifulSoup.\n",
    "\n",
    "    # Extraction des liens par thème\n",
    "    links_bas_carbone = extract_links('collapse-bas-carbone')\n",
    "    links_bim = extract_links('collapse-bim')\n",
    "    # Utilise la fonction extract_links pour extraire les liens des sections \"Bas carbone\" et \"BIM\".\n",
    "\n",
    "    # Thème \"Bas Carbone\"\n",
    "    print(\"Liens de la section 'Bas carbone':\")\n",
    "    bas_carbone_data = []\n",
    "    # Initialise une liste pour stocker les données du thème \"Bas carbone\".\n",
    "\n",
    "    for link in links_bas_carbone:\n",
    "        print(link)\n",
    "        info = get_info(link)\n",
    "        bas_carbone_data.append(info)\n",
    "        # Pour chaque lien extrait, affiche le lien et appelle la fonction get_info pour extraire les informations.\n",
    "        # Ajoute les informations extraites à la liste bas_carbone_data.\n",
    "\n",
    "    # Insertion dans MongoDB pour le thème \"Bas carbone\"\n",
    "    insert_to_db(site_name, \"Bas Carbone\", bas_carbone_data)\n",
    "    # Insère les données du thème \"Bas carbone\" dans la base de données MongoDB.\n",
    "\n",
    "    # Thème \"BIM\"\n",
    "    print(\"\\nLiens de la section 'BIM':\")\n",
    "    bim_data = []\n",
    "    # Initialise une liste pour stocker les données du thème \"BIM\".\n",
    "\n",
    "    for link in links_bim:\n",
    "        print(link)\n",
    "        info = get_info(link)\n",
    "        bim_data.append(info)\n",
    "        # Pour chaque lien extrait, affiche le lien et appelle la fonction get_info pour extraire les informations.\n",
    "        # Ajoute les informations extraites à la liste bim_data.\n",
    "\n",
    "    # Insertion dans MongoDB pour le thème \"BIM\"\n",
    "    insert_to_db(site_name, \"BIM\", bim_data)\n",
    "    # Insère les données du thème \"BIM\" dans la base de données MongoDB.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
