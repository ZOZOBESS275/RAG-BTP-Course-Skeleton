{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module pour l'interaction avec le système de fichiers (gestion de fichiers, répertoires, etc.)\n",
    "import os\n",
    "\n",
    "# Module pour la gestion du temps et des temporisations (sleep, time tracking, etc.)\n",
    "import time\n",
    "\n",
    "# Bibliothèque permettant d'effectuer des requêtes HTTP (GET, POST, etc.)\n",
    "import requests\n",
    "\n",
    "# Bibliothèque Selenium pour automatiser les navigateurs (utile pour le scraping web ou les tests automatisés)\n",
    "from selenium import webdriver\n",
    "\n",
    "# Sous-module de Selenium pour identifier les éléments dans le DOM par leurs attributs (id, class, name, etc.)\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Sous-module Selenium pour gérer les attentes explicites (attendre qu'un élément soit présent, visible, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Sous-module Selenium définissant des conditions attendues (par exemple, présence d'un élément spécifique)\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Importation des exceptions Selenium pour gérer les erreurs fréquentes (éléments non trouvés, délais d'attente, etc.)\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Bibliothèque pour la création et la manipulation de fichiers Excel\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Bibliothèque BeautifulSoup pour analyser et extraire les données du contenu HTML\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Bibliothèque standard pour manipuler les URL et créer des chemins absolus\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Bibliothèque pour interagir avec une base de données MongoDB\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration de dossiers\n",
    "\n",
    "Ce script configure une structure de répertoires pour organiser des fichiers par type (Excel, PDF et audio). \n",
    "Il est couramment utilisé dans des applications où une organisation claire des fichiers est nécessaire avant \n",
    "de procéder à leur manipulation ou leur traitement.\n",
    "\n",
    "**Objectif** : Créer automatiquement les répertoires nécessaires, en s'assurant qu'ils existent avant d'effectuer \n",
    "des opérations de lecture ou d'écriture.\n",
    "\n",
    "**Approche** :\n",
    "1. Définir un dossier principal (`dossier_excel`) pour stocker les fichiers Excel.\n",
    "2. Créer des sous-dossiers pour les types de fichiers connexes (`dossier_pdfs` et `dossier_audios`).\n",
    "3. Utiliser `os.makedirs()` pour chaque répertoire afin de vérifier leur existence avant leur création.\n",
    "\n",
    "**Avantages** :\n",
    "- Évite les erreurs de chemin manquant lors de l'accès à des fichiers.\n",
    "- Simplifie la gestion des fichiers en les organisant de manière hiérarchique et structurée.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `os` doit être disponible (installé par défaut avec Python).\n",
    "- Le dossier de base (`dossier_excel`) sera automatiquement créé s'il n'existe pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du dossier principal pour les fichiers Excel\n",
    "dossier_excel = 'Fichiers_Excel'\n",
    "\n",
    "# Création d'un sous-dossier \"PDFs\" à l'intérieur de \"Fichiers_Excel\"\n",
    "dossier_pdfs = os.path.join(dossier_excel, 'PDFs')\n",
    "\n",
    "# Création d'un sous-dossier \"Audios\" à l'intérieur de \"Fichiers_Excel\"\n",
    "dossier_audios = os.path.join(dossier_excel, 'Audios')\n",
    "\n",
    "# Création du répertoire principal \"Fichiers_Excel\" s'il n'existe pas encore\n",
    "os.makedirs(dossier_excel, exist_ok=True)\n",
    "\n",
    "# Création du sous-dossier \"PDFs\" s'il n'existe pas encore\n",
    "os.makedirs(dossier_pdfs, exist_ok=True)\n",
    "\n",
    "# Création du sous-dossier \"Audios\" s'il n'existe pas encore\n",
    "os.makedirs(dossier_audios, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion à la base de données\n",
    "\n",
    "Ce script établit une connexion à une base de données MongoDB. \n",
    "Il configure la connexion, choisit la base de données, puis sélectionne une collection spécifique où les opérations de lecture et d'écriture seront effectuées.\n",
    "\n",
    "**Objectif** : Connecter le programme à une instance MongoDB pour pouvoir interagir avec les données de manière structurée.\n",
    "\n",
    "**Contexte** : Utilisé dans des applications nécessitant une interaction fréquente avec une base de données MongoDB, \n",
    "par exemple pour stocker, modifier, ou récupérer des documents de manière efficace.\n",
    "\n",
    "**Approche** :\n",
    "1. Initialiser un client MongoDB en fournissant l'URI (Uniform Resource Identifier) de connexion.\n",
    "2. Spécifier la base de données (`db`) sur laquelle le programme va opérer.\n",
    "3. Choisir la collection (`collection`) où seront effectuées les opérations de stockage et de lecture.\n",
    "\n",
    "**Avantages** :\n",
    "- Centralise la gestion des données dans un système de base de données robuste.\n",
    "- Permet un accès rapide et structuré aux informations via des requêtes MongoDB.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `pymongo` doit être installé (`pip install pymongo`).\n",
    "- Un compte MongoDB avec les droits d'accès à la base de données.\n",
    "- L'URI de connexion doit être correctement formaté et sécurisé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la connexion à MongoDB à l'aide de l'URI fourni\n",
    "client = MongoClient('mongodb+srv://serginemengue46:tu3uF7Ap0g2RQDou@cluster0.7xuvx.mongodb.net')  \n",
    "# URI de connexion : contient les informations de l'utilisateur, le mot de passe, et l'hôte (cluster MongoDB).\n",
    "\n",
    "# Sélection de la base de données MongoDB spécifique\n",
    "db = client['nom_de_ta_base']  # Remplacer par le nom de la base de données que vous souhaitez utiliser.\n",
    "\n",
    "# Sélection de la collection \"Proreno\" dans la base de données choisie\n",
    "collection = db['Proreno']  # La collection Proreno est où les documents seront insérés et consultés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options du navigateur pour Chrome\n",
    "\n",
    "Ce script configure les options pour le navigateur Chrome avant de lancer une instance automatisée via Selenium. \n",
    "Ces options personnalisent le comportement du navigateur pour optimiser le processus d'automatisation et éviter les interruptions.\n",
    "\n",
    "**Objectif** : Configurer le navigateur Chrome de manière à désactiver certaines fonctionnalités qui pourraient \n",
    "interférer avec l'automatisation (ex: notifications, pop-ups, etc.) et améliorer la stabilité et la vitesse de navigation.\n",
    "\n",
    "**Contexte** : Utilisé dans les projets de scraping web ou de tests automatisés, où il est essentiel de réduire \n",
    "les interruptions du navigateur pour garantir une exécution fluide et fiable.\n",
    "\n",
    "**Approche** :\n",
    "1. Créer un objet `chrome_options` pour spécifier les configurations du navigateur.\n",
    "2. Ajouter plusieurs arguments (`add_argument`) pour désactiver les notifications, les barres d'informations, et autres éléments perturbateurs.\n",
    "3. Utiliser des options expérimentales pour masquer les signes d'automatisation et maximiser la fenêtre de manière dynamique.\n",
    "\n",
    "**Avantages** :\n",
    "- Évite les interruptions liées aux notifications, pop-ups et barres d'informations.\n",
    "- Réduit les risques de détection d'automatisation par certains sites.\n",
    "- Optimise la performance du navigateur (moins de ressources GPU utilisées).\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé (`pip install selenium`).\n",
    "- Le driver Chrome (`chromedriver`) doit être installé et accessible dans le `PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un objet `ChromeOptions` pour définir des options personnalisées pour le navigateur Chrome\n",
    "chrome_options = webdriver.ChromeOptions()  # Initialisation de l'objet contenant les configurations du navigateur.\n",
    "\n",
    "# Désactiver les notifications du navigateur (par exemple, les demandes de permissions)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  \n",
    "\n",
    "# Désactiver le blocage des pop-ups pour s'assurer que les fenêtres contextuelles n'interfèrent pas avec les tests\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  \n",
    "\n",
    "# Désactiver l'utilisation du GPU pour minimiser la consommation de ressources\n",
    "chrome_options.add_argument(\"--disable-gpu\")  \n",
    "\n",
    "# Nécessaire pour les environnements Linux (empêche certains problèmes liés au sandboxing)\n",
    "chrome_options.add_argument(\"--no-sandbox\")  \n",
    "\n",
    "# Désactiver les barres d'informations qui indiquent que Chrome est géré par un logiciel automatisé\n",
    "chrome_options.add_argument(\"--disable-infobars\")  \n",
    "\n",
    "# Désactiver toutes les extensions du navigateur pour éviter les interférences lors de l'automatisation\n",
    "chrome_options.add_argument(\"--disable-extensions\")  \n",
    "\n",
    "# Désactiver des fonctionnalités spécifiques liées à la sécurité de l'isolation du site\n",
    "chrome_options.add_argument(\"--disable-features=IsolateOrigins,site-per-process\")  \n",
    "\n",
    "# Démarrer le navigateur en mode maximisé pour avoir une meilleure visibilité de la page\n",
    "chrome_options.add_argument(\"--start-maximized\")  \n",
    "\n",
    "# Masquer l'alerte \"Chrome est contrôlé par un logiciel automatisé\" en excluant certaines options\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  \n",
    "\n",
    "# Désactiver l'extension d'automatisation intégrée à Chrome pour empêcher les sites de détecter Selenium\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour générer un chemin unique pour les fichiers téléchargés\n",
    "\n",
    "Cette fonction génère un chemin unique pour un fichier à télécharger dans un dossier spécifique. \n",
    "Elle vérifie si un fichier du même nom existe déjà dans le répertoire cible et, si c'est le cas, \n",
    "elle modifie le nom en y ajoutant un compteur pour éviter tout conflit.\n",
    "\n",
    "**Objectif** : Garantir que chaque fichier téléchargé a un nom unique pour éviter l'écrasement des fichiers existants.\n",
    "\n",
    "**Contexte** : Utile dans des applications de gestion de fichiers ou de téléchargement, où plusieurs fichiers \n",
    "peuvent avoir le même nom d'origine, ce qui nécessite de les différencier pour éviter les conflits.\n",
    "\n",
    "**Approche** :\n",
    "1. Vérifier si le nom de fichier d'origine (`original_filename`) existe déjà dans le dossier spécifié (`dossier`).\n",
    "2. Si le fichier existe, extraire son nom de base (`base`) et son extension (`extension`).\n",
    "3. Ajouter un compteur (`counter`) pour créer une version unique du nom de fichier (exemple: `nom_1.pdf`, `nom_2.pdf`).\n",
    "4. Répéter jusqu'à obtenir un nom de fichier unique, puis retourner le chemin final.\n",
    "\n",
    "**Avantages** :\n",
    "- Évite l'écrasement accidentel de fichiers avec des noms identiques.\n",
    "- Génère des noms de fichiers lisibles et structurés avec des suffixes numériques.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `os` doit être disponible (installé par défaut avec Python).\n",
    "- Le répertoire cible (`dossier`) doit exister pour éviter les erreurs lors de la création du chemin.\n",
    "\n",
    "**Paramètres**:\n",
    "- `dossier`: Chemin du dossier où le fichier sera enregistré.\n",
    "- `original_filename`: Nom de fichier d'origine.\n",
    "\n",
    "**Retourne**:\n",
    "- `filename`: Chemin complet avec un nom unique pour le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_chemin_fichier(dossier, original_filename):\n",
    "    \n",
    "    # Générer le chemin initial basé sur le nom de fichier d'origine\n",
    "    filename = os.path.join(dossier, original_filename)  # Chemin complet du fichier avec le nom d'origine.\n",
    "\n",
    "    # Vérifier si un fichier avec le même nom existe déjà\n",
    "    if os.path.exists(filename):  \n",
    "        # Séparer le nom de base et l'extension pour préparer une version modifiée du nom\n",
    "        base, extension = os.path.splitext(original_filename)  # Ex: \"rapport.pdf\" devient base=\"rapport\", extension=\".pdf\"\n",
    "\n",
    "        # Initialiser un compteur pour générer des noms uniques\n",
    "        counter = 1  \n",
    "\n",
    "        # Boucle pour trouver un nom de fichier disponible en incrémentant le compteur\n",
    "        while os.path.exists(os.path.join(dossier, f\"{base}_{counter}{extension}\")):\n",
    "            counter += 1  # Incrémenter le compteur jusqu'à ce que le nom de fichier soit disponible\n",
    "\n",
    "        # Générer un nouveau chemin de fichier unique en ajoutant le compteur au nom de base\n",
    "        filename = os.path.join(dossier, f\"{base}_{counter}{extension}\")  \n",
    "\n",
    "    # Retourner le chemin complet du fichier avec un nom unique\n",
    "    return filename  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Refus de cookies\n",
    "\n",
    "Cette fonction est utilisée pour automatiser le refus des cookies lors de la navigation sur des sites web via Selenium. \n",
    "Elle localise le bouton de refus de cookies (souvent libellé \"Tout refuser\") et effectue un clic pour le désactiver.\n",
    "\n",
    "**Objectif** : Assurer que les scripts d'automatisation ou de scraping ne sont pas perturbés par les fenêtres de consentement aux cookies.\n",
    "\n",
    "**Contexte** : Utilisée lors de l'automatisation de la navigation sur des sites qui imposent des choix de consentement (cookies). \n",
    "Ces pop-ups peuvent bloquer l'accès à d'autres éléments de la page, ce qui rend nécessaire leur désactivation.\n",
    "\n",
    "**Approche** :\n",
    "1. Rechercher le bouton correspondant au refus des cookies par son `XPATH`.\n",
    "2. Scroll automatique pour rendre le bouton visible dans la fenêtre du navigateur.\n",
    "3. Attendre que le bouton soit cliquable et exécuter le clic.\n",
    "4. Gérer les erreurs pour éviter les interruptions en cas de problème (exemple : élément introuvable).\n",
    "\n",
    "**Avantages** :\n",
    "- Permet de gagner du temps en évitant l'acceptation manuelle des cookies.\n",
    "- Facilite le scraping sur des sites qui utilisent des pop-ups de consentement pour bloquer l'accès.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé (`pip install selenium`).\n",
    "- Le `WebDriver` doit être correctement configuré et relié au navigateur choisi.\n",
    "- Le bouton de refus de cookies doit être identifiable par le `XPATH` utilisé dans le code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour refuser les cookies sur une page web\n",
    "def click_refuse_cookies(driver):\n",
    "    try:\n",
    "        # Afficher un message dans la console pour indiquer le début de la recherche du bouton de refus des cookies\n",
    "        print(\"Recherche du bouton 'Tout refuser' des cookies\")\n",
    "\n",
    "        # Utiliser WebDriverWait pour attendre la présence du bouton de refus de cookies sur la page.\n",
    "        # Le bouton est identifié par son `XPATH` et son attribut aria-label spécifique.\n",
    "        refuse_button = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//button[@aria-label=\\\"Refuser l'utilisation de cookies et d'autres données aux fins décrites\\\"]\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Exécuter un script JavaScript pour s'assurer que le bouton est visible à l'écran en scrollant vers lui\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", refuse_button)\n",
    "\n",
    "        # Attendre que le bouton devienne cliquable avant d'exécuter le clic\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[@aria-label=\\\"Refuser l'utilisation de cookies et d'autres données aux fins décrites\\\"]\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Cliquer sur le bouton pour refuser l'utilisation des cookies\n",
    "        refuse_button.click()\n",
    "\n",
    "        # Afficher un message indiquant que le clic a été effectué avec succès\n",
    "        print(\"Bouton 'Tout refuser' des cookies cliqué\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si une erreur se produit (ex: bouton introuvable ou délai dépassé), afficher le message d'erreur\n",
    "        print(f\"Erreur lors du refus des cookies : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement d'audios\n",
    "\n",
    "Cette fonction télécharge une liste de fichiers audio à partir d'une série d'URLs, les enregistre localement dans un dossier spécifique,\n",
    "et retourne les chemins d'accès aux fichiers téléchargés.\n",
    "\n",
    "**Objectif** : Récupérer des fichiers audio depuis le web, les enregistrer de manière structurée dans un répertoire local \n",
    "et stocker leurs chemins pour une utilisation ultérieure.\n",
    "\n",
    "**Contexte** : Utilisé dans les applications nécessitant de télécharger des ressources audio, comme des podcasts, des fichiers sonores, \n",
    "ou des narrations, afin de les organiser localement et de pouvoir y accéder facilement.\n",
    "\n",
    "**Approche** :\n",
    "1. Parcourir chaque URL de la liste `liens_audio`.\n",
    "2. Effectuer une requête HTTP pour récupérer le fichier audio.\n",
    "3. Générer un chemin unique dans le dossier local pour éviter les conflits de noms.\n",
    "4. Enregistrer le contenu audio dans le fichier local.\n",
    "5. Gérer les erreurs et continuer le téléchargement pour les fichiers suivants.\n",
    "\n",
    "**Avantages** :\n",
    "- Automatise le téléchargement de fichiers audio depuis différentes sources.\n",
    "- Permet de gérer les erreurs et de continuer même si un fichier ne peut pas être téléchargé.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `requests` doit être installé (`pip install requests`).\n",
    "- La fonction `generer_chemin_fichier` doit être définie au préalable pour éviter les conflits de noms.\n",
    "- Le dossier de destination (`dossier_audios`) doit être préalablement créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour télécharger des fichiers audio depuis une liste d'URLs\n",
    "def telecharger_audio(liens_audio):\n",
    "    chemins_audios = []  # Liste pour stocker les chemins des fichiers audio téléchargés\n",
    "\n",
    "    # Parcourir chaque URL fournie dans la liste de liens audio\n",
    "    for url in liens_audio:\n",
    "        try:\n",
    "            # Envoyer une requête HTTP pour télécharger le contenu du fichier audio\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Vérifier si la requête a réussi (code 200)\n",
    "\n",
    "            # Générer un chemin unique pour le fichier audio dans le dossier spécifié pour éviter les conflits\n",
    "            filename = generer_chemin_fichier(dossier_audios, url.split('/')[-1])  \n",
    "            print(f\"Téléchargement de l'audio depuis {url} et sauvegarde sous : {filename}\")\n",
    "\n",
    "            # Enregistrer le fichier audio localement en mode binaire ('wb')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)  # Écrire le contenu de la réponse dans le fichier local\n",
    "\n",
    "            # Ajouter le chemin du fichier téléchargé à la liste des chemins\n",
    "            chemins_audios.append(filename)  \n",
    "            print(f\"Audio téléchargé : {filename}\")\n",
    "\n",
    "        # Gérer les erreurs lors du téléchargement (ex: requête échouée ou problème de connexion)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du téléchargement de l'audio {url} : {e}\")\n",
    "\n",
    "    # Retourner la liste des chemins des fichiers audio téléchargés\n",
    "    return chemins_audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargemnt de pdf\n",
    "\n",
    "Cette fonction télécharge une liste de fichiers PDF à partir de différentes URLs, les enregistre localement dans un dossier spécifique \n",
    "et retourne les chemins d'accès aux fichiers téléchargés.\n",
    "\n",
    "**Objectif** : Télécharger des fichiers PDF depuis des liens web, les organiser dans un répertoire local et conserver \n",
    "les chemins d'accès pour une utilisation ou un traitement ultérieur.\n",
    "\n",
    "**Contexte** : Utilisée dans des applications nécessitant de récupérer et de stocker des documents PDF (par exemple, des rapports, des études, ou des fichiers de référence) \n",
    "pour un accès structuré.\n",
    "\n",
    "**Approche** :\n",
    "1. Parcourir chaque URL de la liste `liens_pdf`.\n",
    "2. Effectuer une requête HTTP pour récupérer le contenu du fichier PDF.\n",
    "3. Générer un chemin unique dans le dossier de destination pour éviter les conflits de noms.\n",
    "4. Enregistrer le contenu PDF dans un fichier local.\n",
    "5. Gérer les erreurs et continuer à télécharger les fichiers suivants en cas de problème.\n",
    "\n",
    "**Avantages** :\n",
    "- Automatise le téléchargement de plusieurs fichiers PDF en une seule exécution.\n",
    "- Gère les erreurs de téléchargement de façon à ne pas interrompre tout le processus.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `requests` doit être installé (`pip install requests`).\n",
    "- La fonction `generer_chemin_fichier` doit être définie au préalable pour éviter les conflits de noms.\n",
    "- Le dossier de destination (`dossier_pdfs`) doit exister ou être créé avant l'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour télécharger des fichiers PDF depuis une liste d'URLs\n",
    "def telecharger_pdfs(liens_pdf):\n",
    "    chemins_pdfs = []  # Liste pour stocker les chemins des fichiers PDF téléchargés\n",
    "\n",
    "    # Parcourir chaque URL fournie dans la liste de liens PDF\n",
    "    for url in liens_pdf:\n",
    "        try:\n",
    "            # Envoyer une requête HTTP pour télécharger le contenu du fichier PDF\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Vérifier si la requête a réussi (code 200)\n",
    "\n",
    "            # Générer un chemin unique pour le fichier PDF dans le dossier spécifié pour éviter les conflits\n",
    "            filename = generer_chemin_fichier(dossier_pdfs, url.split('/')[-1])  \n",
    "            print(f\"Téléchargement du PDF depuis {url} et sauvegarde sous : {filename}\")\n",
    "\n",
    "            # Enregistrer le fichier PDF localement en mode binaire ('wb')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)  # Écrire le contenu de la réponse dans le fichier local\n",
    "\n",
    "            # Ajouter le chemin du fichier téléchargé à la liste des chemins\n",
    "            chemins_pdfs.append(filename)  \n",
    "            print(f\"PDF téléchargé : {filename}\")\n",
    "\n",
    "        # Gérer les erreurs lors du téléchargement (ex: requête échouée ou problème de connexion)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du téléchargement du PDF {url} : {e}\")\n",
    "\n",
    "    # Retourner la liste des chemins des fichiers PDF téléchargés\n",
    "    return chemins_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation des données dans la BDD\n",
    "\n",
    "Cette fonction exporte des données structurées vers une collection MongoDB. Elle vérifie si un document pour un thème spécifique existe déjà. \n",
    "Si ce n'est pas le cas, elle crée un nouveau document avec ce thème. Ensuite, elle met à jour le document avec les données extraites, \n",
    "ainsi que les chemins des fichiers PDF et audio.\n",
    "\n",
    "**Objectif** : Enregistrer ou mettre à jour des données thématiques dans une base MongoDB, \n",
    "tout en gérant les informations associées telles que les fichiers PDF et audios.\n",
    "\n",
    "**Contexte** : Utilisée dans des applications qui collectent et analysent des données, \n",
    "et qui nécessitent un stockage structuré et accessible pour des traitements ultérieurs.\n",
    "\n",
    "**Approche** :\n",
    "1. Vérifier l'existence d'un document correspondant au thème dans la collection MongoDB.\n",
    "2. Créer un document si le thème n'existe pas.\n",
    "3. Insérer les nouvelles données extraites dans le document existant ou créé.\n",
    "4. Ajouter les chemins des fichiers PDF et audio liés.\n",
    "5. Gérer les erreurs d'insertion ou de mise à jour de la base de données.\n",
    "\n",
    "**Avantages** :\n",
    "- Automatise le stockage des données extraites dans une base de données.\n",
    "- Gère la création de documents et la mise à jour sans duplication.\n",
    "- Permet de lier facilement les chemins de fichiers associés (PDF et audio).\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `pymongo` doit être installé (`pip install pymongo`).\n",
    "- Une connexion valide à une instance MongoDB.\n",
    "- La collection MongoDB (`collection`) doit être préalablement définie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour exporter les données dans MongoDB\n",
    "def exporter_vers_mongodb(donnees, chemins_pdfs, chemins_audios, theme):\n",
    "    try:\n",
    "        # Rechercher si un document pour le thème spécifié existe déjà dans la collection\n",
    "        document = collection.find_one({'theme': theme})\n",
    "        \n",
    "        if not document:\n",
    "            # Si le document n'existe pas, créer un nouveau document pour le thème\n",
    "            document = {\n",
    "                'theme': theme,  # Nom du thème\n",
    "                'data': [],  # Liste vide pour stocker les données extraites\n",
    "                'pdf_paths': [],  # Liste vide pour stocker les chemins des fichiers PDF\n",
    "                'audio_paths': []  # Liste vide pour stocker les chemins des fichiers audio\n",
    "            }\n",
    "            \n",
    "            # Insérer le nouveau document dans la collection et récupérer son ID\n",
    "            theme_doc_id = collection.insert_one(document).inserted_id\n",
    "            print(f\"Nouveau document créé pour le thème '{theme}' avec l'ID : {theme_doc_id}\")\n",
    "        else:\n",
    "            # Si le document existe, récupérer l'ID du document\n",
    "            theme_doc_id = document['_id']\n",
    "\n",
    "        # Ajouter les nouvelles données extraites au document correspondant au thème\n",
    "        for line in donnees:\n",
    "            collection.update_one(\n",
    "                {'_id': theme_doc_id},  # Identifier le document par son ID\n",
    "                {'$push': {\n",
    "                    'data': {  # Ajouter un nouvel enregistrement de données\n",
    "                        'Titre': line[0],\n",
    "                        'Description': line[1],\n",
    "                        'Résumé': line[2],\n",
    "                        'Nombre_de_téléchargements': line[3],\n",
    "                        'Nombre_de_likes': line[4],\n",
    "                        'Nombre_d_avis': line[5],\n",
    "                        'Nombre_d_ecoutes': line[6],\n",
    "                        'Nombre_de_vues': line[7],\n",
    "                        'Nombre_de_pages': line[8],\n",
    "                        'Type_de_chantier': line[9],\n",
    "                        'Type_de_bâtiment': line[10],\n",
    "                        'Lots_impliqués': line[11],\n",
    "                        'Sujets_techniques_associés': line[12],\n",
    "                        'Étape_de_chantier': line[13],\n",
    "                        'Contributeur': line[14],\n",
    "                        'Lien': line[15],\n",
    "                        'Lien_video': line[16],\n",
    "                        'Titre_video': line[17],\n",
    "                        'Description_video': line[18],\n",
    "                        'Propriétaire_video': line[19],\n",
    "                        'Nombre_de_vues_video': line[20],\n",
    "                        'Nombre_de_commentaires_video': line[21],\n",
    "                        'Date_de_publication_video': line[22],\n",
    "                        'Nombre_de_likes_video': line[23],\n",
    "                        'audio_paths': chemins_audios,  # Chemins des fichiers audios téléchargés\n",
    "                        'pdf_paths': chemins_pdfs  # Chemins des fichiers PDFs téléchargés\n",
    "                    }\n",
    "                }}\n",
    "            )\n",
    "\n",
    "        # Afficher un message indiquant la réussite de l'exportation des données\n",
    "        print(f\"Données pour le thème '{theme}' mises à jour avec succès dans MongoDB.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si une erreur survient, afficher un message d'erreur avec les détails\n",
    "        print(f\"Erreur lors de l'exportation vers MongoDB : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des informations sur une page\n",
    "\n",
    "Cette fonction extrait les informations d'une page web donnée en utilisant Selenium pour naviguer sur la page \n",
    "et BeautifulSoup pour analyser le contenu HTML. Les informations extraites incluent le titre, la description, le résumé, \n",
    "ainsi que d'autres détails pertinents tels que les statistiques (téléchargements, likes, avis, etc.), \n",
    "les types de bâtiments, les lots impliqués, les étapes de chantier et les contributeurs.\n",
    "\n",
    "**Objectif** : Extraire toutes les informations pertinentes d'une page web et les organiser sous forme structurée \n",
    "pour un traitement ultérieur.\n",
    "\n",
    "**Contexte** : Utilisée dans des applications de scraping de données pour obtenir des informations détaillées sur \n",
    "des pages spécifiques, notamment des rapports ou des fiches descriptives de projets.\n",
    "\n",
    "**Approche** :\n",
    "1. Naviguer sur la page spécifiée à l'aide de Selenium.\n",
    "2. Extraire le contenu HTML de la page avec BeautifulSoup.\n",
    "3. Rechercher les différents éléments en utilisant des sélecteurs CSS (`class`, `id`, etc.) et extraire le texte.\n",
    "4. Organiser les informations dans un format structuré (liste de données).\n",
    "5. Gérer les exceptions et afficher les erreurs en cas de problème.\n",
    "\n",
    "**Avantages** :\n",
    "- Capable d'extraire de multiples informations sur une page, y compris les liens vers des ressources multimédias (PDF, audio, vidéo).\n",
    "- Gestion robuste des erreurs et possibilité de récupérer partiellement les données même en cas d'éléments manquants.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé et configuré (`pip install selenium`).\n",
    "- Le module `beautifulsoup4` doit être installé (`pip install beautifulsoup4`).\n",
    "- Le `WebDriver` de Selenium doit être compatible avec le navigateur utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations d'une page web\n",
    "def recuperer_info_de_page(lien, navigateur):\n",
    "    try:\n",
    "        # Charger la page web avec Selenium\n",
    "        navigateur.get(lien)\n",
    "        time.sleep(5)  # Attendre que la page se charge complètement\n",
    "\n",
    "        # Extraire le contenu HTML de la page chargée\n",
    "        page_html = navigateur.page_source\n",
    "        soup = bs(page_html, 'html.parser')  # Analyser le contenu HTML avec BeautifulSoup\n",
    "\n",
    "        # Extraire le titre de la page\n",
    "        titre = soup.find('h1', class_='title-1 relative z-10').get_text(strip=True) if soup.find('h1', class_='title-1 relative z-10') else 'N/A'\n",
    "\n",
    "        # Extraire la description de la page (balise <h2>)\n",
    "        description_element = soup.find('h2', class_='w-full lg:w-[90%] font-light')\n",
    "        description = description_element.string.strip() if description_element else 'N/A'\n",
    "\n",
    "        # Extraire le résumé de la page (balise <p>)\n",
    "        resume_element = soup.find('p', class_='font-light leading-[1.8] whitespace-pre-line')\n",
    "        resume = resume_element.string.strip() if resume_element else 'N/A'\n",
    "\n",
    "        # Extraire les statistiques de téléchargements, likes, avis, etc.\n",
    "        telechargements = 'N/A'\n",
    "        div_telechargements = soup.find('div', class_='flex justify-between items-center lg:justify-start lg:gap-40')\n",
    "        if div_telechargements:\n",
    "            divs_internes = div_telechargements.find_all('div', class_='flex items-center gap-5')\n",
    "            for div_interne in divs_internes:\n",
    "                element_telechargements = div_interne.find('span', class_='text-14 font-semibold')\n",
    "                sibling_texte = element_telechargements.find_next_sibling('span', class_='text-11')\n",
    "                if sibling_texte and sibling_texte.text.strip() == 'téléchargements':\n",
    "                    telechargements = element_telechargements.text.strip()\n",
    "                    break\n",
    "\n",
    "        # Extraire le nombre de likes\n",
    "        likes = 'N/A'\n",
    "        if div_telechargements:\n",
    "            for div_interne in divs_internes:\n",
    "                element_likes = div_interne.find('span', class_='text-14 font-semibold')\n",
    "                sibling_texte = element_likes.find_next_sibling('span', class_='text-11')\n",
    "                if sibling_texte and sibling_texte.text.strip() == \"J'aime\":\n",
    "                    likes = element_likes.text.strip()\n",
    "                    break\n",
    "\n",
    "        # Extraire le nombre d'avis\n",
    "        avis = 'N/A'\n",
    "        lien_avis = soup.find('a', class_='flex items-center gap-5 no-underline group')\n",
    "        if lien_avis:\n",
    "            element_avis = lien_avis.find('span', class_='text-14 font-semibold')\n",
    "            if element_avis:\n",
    "                avis = element_avis.string.strip()\n",
    "\n",
    "        # Extraire le nombre d'écoutes\n",
    "        ecoutes = 'N/A'\n",
    "        ecoutes_element = soup.find('span', class_='icon-sound text-white text-20 mr-[3px]')\n",
    "        if ecoutes_element:\n",
    "            ecoutes_span = ecoutes_element.find_next('span', class_='text-14 font-semibold')\n",
    "            if ecoutes_span:\n",
    "                text_sibling = ecoutes_span.find_next_sibling('span', class_='text-11')\n",
    "                if text_sibling and text_sibling.text.strip() == 'écoutes':\n",
    "                    ecoutes = ecoutes_span.text.strip()\n",
    "\n",
    "        # Extraire le nombre de vues\n",
    "        vues = 'N/A'\n",
    "        vues_element = soup.find('span', class_='icon-play text-white text-20 mr-[3px]')\n",
    "        if vues_element:\n",
    "            vues_span = vues_element.find_next('span', class_='text-14 font-semibold')\n",
    "            if vues_span:\n",
    "                text_sibling = vues_span.find_next_sibling('span', class_='text-11')\n",
    "                if text_sibling and text_sibling.text.strip() == 'vues':\n",
    "                    vues = vues_span.text.strip()\n",
    "\n",
    "        # Extraire le nombre de pages du document\n",
    "        nombre_de_pages = 'N/A'\n",
    "        div_nombre_de_pages = soup.find('div', class_='flex items-center gap-5')\n",
    "        if div_nombre_de_pages:\n",
    "            spans = div_nombre_de_pages.find_all('span', class_='text-14 font-semibold')\n",
    "            for span in spans:\n",
    "                sibling_texte = span.find_next_sibling('span', class_='text-11')\n",
    "                if sibling_texte and sibling_texte.text.strip() == 'pages':\n",
    "                    nombre_de_pages = span.text.strip()\n",
    "                    break\n",
    "\n",
    "        # Extraire les informations sur le type de chantier\n",
    "        type_de_chantier = 'N/A'\n",
    "        div_type_de_chantier = soup.find('div', class_='flex items-center gap-10 flex-wrap')\n",
    "        if div_type_de_chantier:\n",
    "            liens_chantier = div_type_de_chantier.find_all('a', class_='no-underline')\n",
    "            type_de_chantier = ', '.join([lien.text.strip() for lien in liens_chantier]) if liens_chantier else 'N/A'\n",
    "\n",
    "        # Extraire les informations sur le type de bâtiment\n",
    "        type_de_batiment = 'N/A'\n",
    "        divs_type_de_batiment = soup.find_all('div', class_='flex items-center gap-10 flex-wrap')\n",
    "        if len(divs_type_de_batiment) > 1:\n",
    "            liens_batiment = divs_type_de_batiment[1].find_all('a', class_='no-underline')\n",
    "            type_de_batiment = ', '.join([lien.text.strip() for lien in liens_batiment]) if liens_batiment else 'N/A'\n",
    "\n",
    "        # Extraire les informations sur les lots impliqués\n",
    "        lots_impliques = 'N/A'\n",
    "        if len(divs_type_de_batiment) > 2:\n",
    "            liens_lots = divs_type_de_batiment[2].find_all('a', class_='no-underline')\n",
    "            lots_impliques = ', '.join([lien.text.strip() for lien in liens_lots]) if liens_lots else 'N/A'\n",
    "\n",
    "        # Extraire les sujets techniques associés\n",
    "        sujets_techniques_associes = 'N/A'\n",
    "        if len(divs_type_de_batiment) > 3:\n",
    "            liens_sujets = divs_type_de_batiment[3].find_all('a', class_='no-underline')\n",
    "            sujets_techniques_associes = ', '.join([lien.text.strip() for lien in liens_sujets]) if liens_sujets else 'N/A'\n",
    "\n",
    "        # Extraire l'étape de chantier\n",
    "        etape_de_chantier = 'N/A'\n",
    "        ul_etape_de_chantier = soup.find('ul', class_='flex justify-start items-center gap-30 flex-wrap')\n",
    "        if ul_etape_de_chantier:\n",
    "            p_etapes = ul_etape_de_chantier.find_all('p')\n",
    "            etape_de_chantier = ', '.join([p.text.strip() for p in p_etapes]) if p_etapes else 'N/A'\n",
    "\n",
    "        # Extraire les informations sur les contributeurs\n",
    "        contributeur = 'N/A'\n",
    "        div_contributeur = soup.find('div', class_='flex flex-col gap-10 py-10')\n",
    "        if div_contributeur:\n",
    "            p_elements = div_contributeur.find_all('p')\n",
    "            contributeur = '\\n'.join([p.text.strip() for p in p_elements]) if p_elements else 'N/A'\n",
    "\n",
    "        # Extraire les liens vers les fichiers PDF\n",
    "        liens_pdf = []\n",
    "        pdf_link = soup.find('a', id='downloadLink')\n",
    "        if pdf_link and pdf_link['href'].lower().endswith('.pdf'):\n",
    "            full_url = urljoin(lien, pdf_link['href'])\n",
    "            liens_pdf.append(full_url)\n",
    "\n",
    "        # Extraire les liens vers les fichiers audio\n",
    "        liens_audio = []\n",
    "        audio_elements = soup.find_all('audio', preload=\"metadata\")\n",
    "        for audio in audio_elements:\n",
    "            if 'src' in audio.attrs:\n",
    "                full_url = urljoin(lien, audio['src'])\n",
    "                liens_audio.append(full_url)\n",
    "\n",
    "        # Récupérer les informations vidéo (si disponibles)\n",
    "        infos_video = []\n",
    "        liens_video = []\n",
    "        div_videos = soup.find('dialog', id='video-popin')\n",
    "        if div_videos:\n",
    "            iframe_elements = div_videos.find_all('iframe', src=True)\n",
    "            for iframe in iframe_elements:\n",
    "                src = iframe['src']\n",
    "                full_url = urljoin(lien, src)\n",
    "                if 'youtube-nocookie.com/embed/' in full_url:\n",
    "                    video_id = full_url.split('youtube-nocookie.com/embed/')[1]\n",
    "                    youtube_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "                    liens_video.append(youtube_url)\n",
    "                    infos_video.append(extraire_infos_youtube(navigateur, youtube_url))\n",
    "\n",
    "        # Si aucune information vidéo n'est trouvée, initialiser avec des valeurs par défaut\n",
    "        infos_video = infos_video[0] if infos_video else ['N/A'] * 7\n",
    "        if not liens_video:\n",
    "            liens_video = [\"N/A\"]\n",
    "\n",
    "        # Afficher un message indiquant la réussite de l'extraction\n",
    "        print(f\"\\nExtraction des informations pour le lien {lien} réussie.\")\n",
    "\n",
    "        # Organiser les informations extraites dans une liste\n",
    "        info = [\n",
    "            titre,\n",
    "            description,\n",
    "            resume,\n",
    "            telechargements,\n",
    "            likes,\n",
    "            avis,\n",
    "            ecoutes,\n",
    "            vues,\n",
    "            nombre_de_pages,\n",
    "            type_de_chantier,\n",
    "            type_de_batiment,\n",
    "            lots_impliques,\n",
    "            sujets_techniques_associes,\n",
    "            etape_de_chantier,\n",
    "            contributeur,\n",
    "            lien, \n",
    "            ', '.join(liens_video),  # Liens vidéo\n",
    "            infos_video[0],  # Titre vidéo\n",
    "            infos_video[1],  # Description vidéo\n",
    "            infos_video[2],  # Propriétaire vidéo\n",
    "            infos_video[3],  # Vues vidéo\n",
    "            infos_video[4],  # Nombre de commentaires vidéo\n",
    "            infos_video[5],  # Date de publication\n",
    "            infos_video[6],  # Nombre de likes vidéo\n",
    "        ]\n",
    "\n",
    "        # Retourner les informations extraites ainsi que les liens PDF et audio\n",
    "        return info, liens_pdf, liens_audio, liens_video\n",
    "\n",
    "    except Exception as e:\n",
    "        # Afficher un message d'erreur en cas de problème\n",
    "        print(f\"Erreur lors de l'extraction des informations : {str(e)}\")\n",
    "        return None, [], [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des informations sur youtube \n",
    "\n",
    "Cette fonction extrait les informations d'une vidéo YouTube en utilisant Selenium pour naviguer sur la page. \n",
    "Elle collecte des informations telles que le titre de la vidéo, la description, le propriétaire de la chaîne, \n",
    "le nombre de vues, la date de publication, le nombre de commentaires et de likes.\n",
    "\n",
    "**Objectif** : Automatiser l'extraction des informations détaillées d'une vidéo YouTube pour les organiser et les exploiter ultérieurement.\n",
    "\n",
    "**Contexte** : Utilisée pour récupérer les métadonnées des vidéos YouTube, par exemple pour analyser des chaînes, \n",
    "réaliser des études de contenu, ou structurer les informations d'une vidéo particulière.\n",
    "\n",
    "**Approche** :\n",
    "1. Charger la page de la vidéo YouTube à l'aide de Selenium.\n",
    "2. Gérer l'affichage de la page (acceptation/refus des cookies, clic sur \"Afficher plus\").\n",
    "3. Extraire les informations de la vidéo en identifiant les éléments spécifiques du DOM.\n",
    "4. Structurer ces informations sous forme de liste pour une utilisation ou un stockage ultérieur.\n",
    "\n",
    "**Avantages** :\n",
    "- Automatise la récupération de toutes les informations pertinentes d'une vidéo YouTube.\n",
    "- Gestion des exceptions pour continuer le traitement même en cas d'erreur sur certaines informations.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé (`pip install selenium`).\n",
    "- Un WebDriver compatible (par exemple, `chromedriver`) doit être configuré pour le navigateur choisi.\n",
    "- L'accès à la page YouTube doit être disponible sans restrictions (ex : pas de blocage régional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations d'une vidéo YouTube\n",
    "def extraire_infos_youtube(navigateur, lien_video):\n",
    "    try:\n",
    "        # Charger la page de la vidéo YouTube avec Selenium\n",
    "        navigateur.get(lien_video)\n",
    "        time.sleep(10)  # Attendre que la page soit complètement chargée\n",
    "\n",
    "        # Gérer la fenêtre de consentement aux cookies (si présente)\n",
    "        try:\n",
    "            bouton_refuser_cookies = WebDriverWait(navigateur, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/form[1]/div/div/button/span'))\n",
    "            )\n",
    "            bouton_refuser_cookies.click()\n",
    "        except TimeoutException:\n",
    "            pass  # Si le bouton de cookies n'apparaît pas, continuer l'exécution\n",
    "\n",
    "        # Scroller vers le bas pour charger tous les éléments de la page\n",
    "        navigateur.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Attendre que les éléments soient bien chargés\n",
    "\n",
    "        # Afficher la description complète de la vidéo (cliquer sur le bouton \"Afficher plus\" si présent)\n",
    "        try:\n",
    "            bouton_afficher_plus = WebDriverWait(navigateur, 10).until(\n",
    "                EC.element_to_be_clickable((By.ID, 'expand'))\n",
    "            )\n",
    "            bouton_afficher_plus.click()\n",
    "        except TimeoutException:\n",
    "            pass  # Si le bouton n'est pas trouvé, continuer\n",
    "\n",
    "        # Extraire le titre de la vidéo\n",
    "        try:\n",
    "            titre_video = WebDriverWait(navigateur, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[1]/h1/yt-formatted-string'))\n",
    "            ).text\n",
    "        except:\n",
    "            titre_video = \"N/A\"\n",
    "\n",
    "        # Extraire la description de la vidéo\n",
    "        try:\n",
    "            description_video = WebDriverWait(navigateur, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[4]/div[1]/div/ytd-text-inline-expander/yt-attributed-string'))\n",
    "            ).text\n",
    "        except:\n",
    "            description_video = \"N/A\"\n",
    "\n",
    "        # Extraire le nom du propriétaire de la chaîne\n",
    "        try:\n",
    "            proprietaire_video = WebDriverWait(navigateur, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'yt-formatted-string.style-scope.ytd-channel-name'))\n",
    "            ).text\n",
    "        except:\n",
    "            proprietaire_video = \"N/A\"\n",
    "\n",
    "        # Extraire le nombre de vues de la vidéo\n",
    "        try:\n",
    "            vues_video = WebDriverWait(navigateur, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[4]/div[1]/div/ytd-watch-info-text/div/yt-formatted-string/span[1]'))\n",
    "            ).text\n",
    "        except:\n",
    "            vues_video = \"N/A\"\n",
    "\n",
    "        # Extraire la date de publication de la vidéo\n",
    "        try:\n",
    "            date_publication = WebDriverWait(navigateur, 20).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[4]/div[1]/div/ytd-watch-info-text/div/yt-formatted-string/span[3]'))\n",
    "            ).text\n",
    "        except:\n",
    "            date_publication = \"N/A\"\n",
    "\n",
    "        # Extraire le nombre de commentaires de la vidéo\n",
    "        try:\n",
    "            nombre_commentaires = WebDriverWait(navigateur, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'yt-formatted-string.count-text.style-scope.ytd-comments-header-renderer'))\n",
    "            ).text\n",
    "        except:\n",
    "            nombre_commentaires = \"N/A\"\n",
    "\n",
    "        # Extraire le nombre de likes de la vidéo\n",
    "        try:\n",
    "            nombre_likes = WebDriverWait(navigateur, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/div/div[2]/div[2]/div/div/ytd-menu-renderer/div[1]/segmented-like-dislike-button-view-model/yt-smartimation/div/div/like-button-view-model/toggle-button-view-model/button-view-model/button/div[2]'))\n",
    "            ).text\n",
    "        except:\n",
    "            nombre_likes = \"N/A\"\n",
    "\n",
    "        # Organiser toutes les informations dans une liste\n",
    "        infos_video = [\n",
    "            titre_video,  # Titre de la vidéo\n",
    "            description_video,  # Description de la vidéo\n",
    "            proprietaire_video,  # Propriétaire de la chaîne\n",
    "            vues_video,  # Nombre de vues de la vidéo\n",
    "            nombre_commentaires,  # Nombre de commentaires de la vidéo\n",
    "            date_publication,  # Date de publication de la vidéo\n",
    "            nombre_likes  # Nombre de likes de la vidéo\n",
    "        ]\n",
    "\n",
    "        # Retourner les informations extraites\n",
    "        return infos_video\n",
    "\n",
    "    except Exception as e:\n",
    "        # Afficher un message d'erreur si l'extraction échoue\n",
    "        print(f\"Erreur lors de l'extraction des informations de la vidéo YouTube : {str(e)}\")\n",
    "        return ['N/A'] * 7  # Retourner des valeurs par défaut en cas d'échec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des informations sur toutes les pages\n",
    "\n",
    "Cette fonction gère l'extraction, le téléchargement et l'exportation des données d'une liste de liens web. \n",
    "Elle commence par extraire les informations de chaque page web spécifiée, puis télécharge les fichiers PDF et audio associés. \n",
    "Enfin, elle exporte toutes les données collectées vers MongoDB sous le thème spécifié.\n",
    "\n",
    "**Objectif** : Extraire les informations de chaque lien, télécharger les fichiers associés, et organiser \n",
    "les données pour un stockage structuré dans une base de données MongoDB.\n",
    "\n",
    "**Contexte** : Utilisée pour automatiser le scraping de pages multiples, récupérer des fichiers multimédias \n",
    "associés, et centraliser le tout dans une base de données afin de faciliter la gestion et l'analyse ultérieure.\n",
    "\n",
    "**Approche** :\n",
    "1. Parcourir chaque lien et extraire les informations de la page.\n",
    "2. Collecter les chemins des fichiers PDF et audio associés.\n",
    "3. Télécharger les fichiers PDF et audio s'ils sont présents.\n",
    "4. Exporter toutes les informations extraites et les chemins de fichiers vers MongoDB.\n",
    "\n",
    "**Avantages** :\n",
    "- Gère l'extraction, le téléchargement et le stockage dans MongoDB en une seule fonction.\n",
    "- Automatise le processus de collecte de données, réduisant le temps et les efforts manuels.\n",
    "\n",
    "**Prérequis** :\n",
    "- Les fonctions `recuperer_info_de_page`, `telecharger_pdfs`, `telecharger_audio` et `exporter_vers_mongodb` doivent être définies.\n",
    "- La collection MongoDB doit être prête à recevoir les données extraites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations des pages après avoir récupéré les liens\n",
    "def extraire_donnees_apres_liens(navigateur, tous_liens, theme):\n",
    "    toutes_donnees = []  # Liste pour stocker toutes les données extraites des pages web\n",
    "    tous_liens_pdf = []  # Liste pour collecter les liens des fichiers PDF\n",
    "    tous_liens_audio = []  # Liste pour collecter les liens des fichiers audio\n",
    "\n",
    "    # Parcourir chaque lien dans la liste des liens fournis\n",
    "    for lien in tous_liens:\n",
    "        # Extraire les informations de la page ainsi que les liens vers les fichiers PDF et audio\n",
    "        info, liens_pdf, liens_audio, _ = recuperer_info_de_page(lien, navigateur)\n",
    "        if info:\n",
    "            toutes_donnees.append(info)  # Ajouter les informations extraites à la liste `toutes_donnees`\n",
    "        tous_liens_pdf.extend(liens_pdf)  # Ajouter les liens PDF à la liste globale `tous_liens_pdf`\n",
    "        tous_liens_audio.extend(liens_audio)  # Ajouter les liens audio à la liste globale `tous_liens_audio`\n",
    "\n",
    "    # Télécharger les fichiers PDF si des liens ont été collectés\n",
    "    chemins_pdfs = telecharger_pdfs(tous_liens_pdf) if tous_liens_pdf else []  # Récupérer les chemins des fichiers PDF téléchargés\n",
    "\n",
    "    # Télécharger les fichiers audio si des liens ont été collectés\n",
    "    chemins_audios = telecharger_audio(tous_liens_audio) if tous_liens_audio else []  # Récupérer les chemins des fichiers audio téléchargés\n",
    "\n",
    "    # Exporter toutes les données extraites vers MongoDB si la liste `toutes_donnees` n'est pas vide\n",
    "    if toutes_donnees:\n",
    "        print(f\"{len(toutes_donnees)} enregistrements prêts à être exportés dans MongoDB.\")  # Afficher le nombre d'enregistrements à exporter\n",
    "        exporter_vers_mongodb(toutes_donnees, chemins_pdfs, chemins_audios, theme)  # Exporter les données vers MongoDB sous le thème spécifié"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupréation de liens sur une page\n",
    "\n",
    "Cette fonction récupère les liens des éléments présents sur une page web. Elle utilise Selenium pour naviguer sur la page \n",
    "et BeautifulSoup pour analyser le contenu HTML. Elle recherche spécifiquement les liens dans une section précise \n",
    "de la page, les convertit en URLs complètes, puis les retourne sous forme de liste.\n",
    "\n",
    "**Objectif** : Extraire tous les liens pertinents d'une section d'une page web afin de les utiliser \n",
    "pour l'extraction d'informations ultérieure.\n",
    "\n",
    "**Contexte** : Utilisée dans des applications de scraping pour récupérer des liens menant à d'autres pages de détails, \n",
    "par exemple pour extraire les informations de projets ou d'articles listés sur une page principale.\n",
    "\n",
    "**Approche** :\n",
    "1. Analyser le contenu HTML de la page chargée à l'aide de BeautifulSoup.\n",
    "2. Identifier la section contenant les liens d'intérêt en utilisant un sélecteur CSS précis.\n",
    "3. Extraire tous les éléments `<a>` contenant des liens (`href`) et construire les URLs complètes.\n",
    "4. Retourner la liste des liens trouvés.\n",
    "\n",
    "**Avantages** :\n",
    "- Simplifie la récupération de tous les liens pertinents d'une page structurée.\n",
    "- Gère les erreurs pour éviter l'interruption de l'extraction en cas de problème.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `beautifulsoup4` doit être installé (`pip install beautifulsoup4`).\n",
    "- Un navigateur automatisé (`navigateur`) doit être configuré avec Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour récupérer les liens d'une page web\n",
    "def recuperer_liens_de_page(navigateur):\n",
    "    try:\n",
    "        # Extraire le contenu HTML de la page actuelle\n",
    "        page_html = navigateur.page_source  \n",
    "        soup = bs(page_html, 'html.parser')  # Analyser le contenu HTML avec BeautifulSoup\n",
    "\n",
    "        # Sélectionner la section spécifique contenant les liens (en utilisant un sélecteur CSS)\n",
    "        section = soup.select_one('div.grid.gap-20.grid-cols-2.sm\\\\:grid-cols-3.xl\\\\:gap-30')\n",
    "        \n",
    "        liens = []  # Initialiser une liste vide pour stocker les liens extraits\n",
    "\n",
    "        # Si la section est trouvée, rechercher tous les éléments <a> contenant des liens\n",
    "        if section:\n",
    "            elements_liens = section.find_all('a', href=True)  # Trouver tous les éléments <a> avec l'attribut `href`\n",
    "            for element in elements_liens:\n",
    "                url = element['href']  # Extraire le lien de l'attribut `href`\n",
    "                # Convertir l'URL relative en URL absolue en utilisant l'URL actuelle de la page\n",
    "                full_url = urljoin(navigateur.current_url, url)\n",
    "                liens.append(full_url)  # Ajouter l'URL complète à la liste `liens`\n",
    "\n",
    "        # Retourner la liste complète des liens trouvés sur la page\n",
    "        return liens\n",
    "\n",
    "    except Exception as e:\n",
    "        # Afficher un message d'erreur si un problème survient lors de la récupération des liens\n",
    "        print(f\"Erreur lors de la récupération des liens sur la page : {str(e)}\")\n",
    "        return []  # Retourner une liste vide en cas d'erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination et récupération de liens\n",
    "\n",
    "Cette fonction gère la pagination d'un site web et récupère les liens de chaque page. Elle utilise Selenium pour naviguer \n",
    "à travers plusieurs pages et BeautifulSoup pour extraire les liens d'intérêt. La fonction continue à parcourir les pages \n",
    "tant qu'un bouton \"Suivant\" est disponible, puis compile tous les liens extraits dans une liste.\n",
    "\n",
    "**Objectif** : Récupérer tous les liens disponibles sur plusieurs pages d'un site comportant un système de pagination.\n",
    "\n",
    "**Contexte** : Utilisée pour automatiser le scraping de sites avec de multiples pages de résultats, \n",
    "par exemple pour des catalogues de produits, des listes de projets, ou des articles répartis sur plusieurs pages.\n",
    "\n",
    "**Approche** :\n",
    "1. Charger la première page et attendre que les éléments de la page soient visibles.\n",
    "2. Extraire les liens de chaque page à l'aide de `recuperer_liens_de_page`.\n",
    "3. Vérifier la présence du bouton \"Suivant\" et naviguer vers la page suivante.\n",
    "4. Répéter jusqu'à atteindre la dernière page, puis retourner la liste complète des liens.\n",
    "\n",
    "**Avantages** :\n",
    "- Automatisation complète de la navigation sur des pages paginées.\n",
    "- Extraction fiable de tous les liens, même sur des sites à chargement dynamique.\n",
    "- Gère les erreurs pour éviter l'interruption prématurée de l'extraction.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé (`pip install selenium`).\n",
    "- La fonction `recuperer_liens_de_page` doit être définie pour extraire les liens de chaque page.\n",
    "- Un WebDriver Selenium compatible doit être configuré et accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour gérer la pagination et récupérer les liens de toutes les pages\n",
    "def extraire_tous_les_liens(navigateur):\n",
    "    tous_liens = []  # Liste pour stocker tous les liens récupérés de chaque page\n",
    "    page = 1  # Compteur de pages, initialisé à 1\n",
    "\n",
    "    # Boucle pour parcourir toutes les pages jusqu'à atteindre la dernière\n",
    "    while True:\n",
    "        try:\n",
    "            # Attendre que les éléments de la grille des résultats soient chargés sur la page\n",
    "            WebDriverWait(navigateur, 40).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.grid.gap-20.grid-cols-2.sm\\\\:grid-cols-3.xl\\\\:gap-30'))\n",
    "            )\n",
    "            print(f\"Résultats de la page {page} visibles\")  # Confirmer que la page est chargée\n",
    "\n",
    "            # Extraire les liens de la page actuelle en utilisant la fonction `recuperer_liens_de_page`\n",
    "            liens_page = recuperer_liens_de_page(navigateur)\n",
    "            tous_liens.extend(liens_page)  # Ajouter les liens récupérés à la liste `tous_liens`\n",
    "            print(f\"Liens récupérés pour la page {page} : {liens_page}\")  # Afficher les liens récupérés\n",
    "\n",
    "            # Rechercher le bouton \"Suivant\" pour passer à la page suivante\n",
    "            try:\n",
    "                bouton_suivant = WebDriverWait(navigateur, 20).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'nav.flex.gap-10 a:last-child'))  # Sélectionner le dernier bouton de navigation\n",
    "                )\n",
    "\n",
    "                # Vérifier si le bouton \"Suivant\" est désactivé, indiquant la dernière page\n",
    "                if \"disabled\" in bouton_suivant.get_attribute(\"class\"):\n",
    "                    print(\"Dernière page atteinte.\")  # Afficher un message si la dernière page est atteinte\n",
    "                    break  # Sortir de la boucle si aucune page suivante n'est disponible\n",
    "                else:\n",
    "                    # Récupérer le lien vers la page suivante et naviguer vers celle-ci\n",
    "                    lien_suivant = bouton_suivant.get_attribute('href')\n",
    "                    navigateur.get(lien_suivant)  # Charger la page suivante\n",
    "                    page += 1  # Incrémenter le numéro de page\n",
    "                    time.sleep(3)  # Attendre que la page soit chargée avant de continuer\n",
    "\n",
    "            # Gérer les exceptions si le bouton \"Suivant\" est introuvable ou si un problème survient lors du clic\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                print(\"Erreur : Impossible de trouver ou cliquer sur le bouton 'Suivant'.\")\n",
    "                break  # Sortir de la boucle si le bouton \"Suivant\" n'est pas disponible ou cliquable\n",
    "\n",
    "        # Gérer l'exception si le chargement de la page prend trop de temps\n",
    "        except TimeoutException:\n",
    "            print(f\"Temps d'attente dépassé pour la page {page}\")  # Afficher un message d'erreur pour la page en question\n",
    "            break  # Sortir de la boucle en cas de délai d'attente dépassé\n",
    "\n",
    "    # Retourner la liste de tous les liens collectés\n",
    "    return tous_liens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche par thème\n",
    "\n",
    "Cette fonction effectue une recherche sur le site `https://www.proreno.fr/` pour un thème donné. \n",
    "Elle utilise Selenium pour naviguer sur la page, entrer le thème dans la barre de recherche, puis cliquer \n",
    "sur le bouton de recherche. Une fois la recherche effectuée, la fonction lance la récupération des liens associés \n",
    "et l'extraction des données pour le thème.\n",
    "\n",
    "**Objectif** : Automatiser la recherche sur un site web pour différents thèmes et extraire les informations \n",
    "et les liens pertinents associés.\n",
    "\n",
    "**Contexte** : Utilisée pour automatiser la recherche de thématiques spécifiques sur des sites web, récupérer \n",
    "toutes les pages de résultats, et extraire les données détaillées pour une analyse ultérieure.\n",
    "\n",
    "**Approche** :\n",
    "1. Accéder à la page d'accueil du site.\n",
    "2. Entrer le thème spécifié dans la barre de recherche et cliquer sur le bouton de recherche.\n",
    "3. Attendre que la page de résultats se charge.\n",
    "4. Parcourir les résultats pour récupérer tous les liens et extraire les informations associées.\n",
    "\n",
    "**Avantages** :\n",
    "- Permet de traiter plusieurs thèmes en une seule exécution.\n",
    "- Gère les erreurs potentielles lors de la recherche et de la navigation.\n",
    "- Automatisation complète du processus de recherche, récupération de liens et extraction de données.\n",
    "\n",
    "**Prérequis** :\n",
    "- Le module `selenium` doit être installé (`pip install selenium`).\n",
    "- Un WebDriver (comme `chromedriver`) doit être configuré et accessible.\n",
    "- Les fonctions `extraire_tous_les_liens` et `extraire_donnees_apres_liens` doivent être définies au préalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour effectuer une recherche sur le site pour un thème donné\n",
    "def effectuer_recherche(navigateur, theme):\n",
    "    try:\n",
    "        # Accéder à la page d'accueil du site Proreno\n",
    "        navigateur.get(\"https://www.proreno.fr/\")\n",
    "        \n",
    "        # Localiser la barre de recherche par son ID et attendre qu'elle soit visible\n",
    "        barre_recherche = WebDriverWait(navigateur, 20).until(\n",
    "            EC.visibility_of_element_located((By.ID, 'autocomplete-input'))\n",
    "        )\n",
    "        barre_recherche.clear()  # Vider le champ de recherche au cas où il contient déjà du texte\n",
    "        time.sleep(1)  # Pause courte pour assurer la stabilité avant d'entrer le texte\n",
    "\n",
    "        # Entrer le thème spécifié dans la barre de recherche\n",
    "        barre_recherche.send_keys(theme)\n",
    "\n",
    "        # Localiser le bouton de recherche par ses attributs CSS et attendre qu'il soit cliquable\n",
    "        bouton_recherche = WebDriverWait(navigateur, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-v-89374d9f][class*=btn-rounded][class*=search-btn]'))\n",
    "        )\n",
    "        \n",
    "        # Utiliser un script JavaScript pour cliquer sur le bouton de recherche\n",
    "        navigateur.execute_script(\"arguments[0].click();\", bouton_recherche)\n",
    "        time.sleep(3)  # Pause courte pour laisser le temps à la page de résultats de se charger\n",
    "\n",
    "    except Exception as e:\n",
    "        # Gérer les exceptions et afficher un message d'erreur en cas de problème\n",
    "        print(f\"Erreur lors de la recherche pour le thème '{theme}' : {str(e)}\")\n",
    "\n",
    "\n",
    "# Initialiser le navigateur avec les options Chrome configurées\n",
    "navigateur = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Liste des thèmes à rechercher\n",
    "themes = [\"Métiers de l'encadrement\", \"Transition environnementale\"]\n",
    "\n",
    "# Boucle principale pour traiter chaque thème de la liste\n",
    "try:\n",
    "    for theme in themes:\n",
    "        # Effectuer une recherche pour le thème actuel\n",
    "        effectuer_recherche(navigateur, theme)\n",
    "        \n",
    "        # Étape 1 : Récupérer tous les liens pour toutes les pages de résultats de la recherche\n",
    "        tous_liens = extraire_tous_les_liens(navigateur)\n",
    "        print(f\"Nombre total de liens récupérés pour le thème '{theme}' : {len(tous_liens)}\")\n",
    "\n",
    "        # Étape 2 : Extraire les données après avoir récupéré tous les liens collectés\n",
    "        extraire_donnees_apres_liens(navigateur, tous_liens, theme)\n",
    "\n",
    "# Fermer le navigateur et libérer les ressources après l'exécution\n",
    "finally:\n",
    "    navigateur.quit()  # Fermer le navigateur une fois toutes les recherches terminées\n",
    "    print(\"Navigateur fermé.\")  # Afficher un message indiquant la fermeture du navigateur\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
