{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Importation de la bibliothèque requests pour envoyer des requêtes HTTP et récupérer le contenu des pages web.\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# Importation de BeautifulSoup depuis bs4 pour faciliter le traitement et le parsing du HTML.\n",
    "\n",
    "import os\n",
    "# Importation du module os pour interagir avec le système de fichiers (création de dossiers, manipulation de chemins).\n",
    "\n",
    "from openpyxl import Workbook\n",
    "# Importation de Workbook depuis openpyxl pour créer et manipuler des fichiers Excel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création dossiers pour enregistrer les fichiers\n",
    "\n",
    "Ces lignes créent des dossiers spécifiques pour organiser les différents types de fichiers téléchargés ou générés par le script.\n",
    "\n",
    "Variables de dossiers :\n",
    "\n",
    "- `excel_folder` : Dossier pour stocker les fichiers Excel contenant des données extraites.\n",
    "- ``image_folder`` : Dossier pour stocker les images téléchargées depuis les pages web.\n",
    "- ``pdf_folder`` : Dossier pour stocker les fichiers PDF téléchargés.\n",
    "\n",
    "- ``os.makedirs(path, exist_ok=True)`` :\n",
    "\n",
    "- ``path`` : Définit le chemin du dossier à créer.\n",
    "- ``exist_ok=True`` : Permet d'éviter une erreur si le dossier existe déjà. Le script vérifie donc l'existence du dossier avant d'essayer de le créer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_folder = 'Fichiers_Excel'\n",
    "image_folder = 'Images'\n",
    "pdf_folder = 'PDFs'\n",
    "# Définition des noms des dossiers pour stocker les fichiers Excel, les images et les PDF.\n",
    "\n",
    "os.makedirs(excel_folder, exist_ok=True)  # Créer le dossier Excel s'il n'existe pas\n",
    "os.makedirs(image_folder, exist_ok=True)  # Créer le dossier Images s'il n'existe pas\n",
    "os.makedirs(pdf_folder, exist_ok=True)    # Créer le dossier PDFs s'il n'existe pas\n",
    "# Création de chaque dossier avec os.makedirs et vérification d'existence (exist_ok=True).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du texte\n",
    "\n",
    "La fonction ``clean_text`` prend en entrée une chaîne de texte brute qui peut contenir des caractères spéciaux ou des balises HTML. Elle utilise des méthodes de remplacement et BeautifulSoup pour transformer le texte en une version propre et lisible.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``text`` : Chaîne de caractères à nettoyer, pouvant inclure des balises HTML et des caractères spéciaux.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie le texte nettoyé sous forme de chaîne de caractères, sans balises HTML, avec des sauts de ligne pour chaque élément de liste <li>.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Remplace les caractères spéciaux ``\\xa0`` (espaces insécables) par des espaces standard pour uniformiser le texte.\n",
    "- Ajoute des sauts de ligne après les balises ``<li>`` pour améliorer la lisibilité de chaque élément de liste dans le texte extrait.\n",
    "- Utilise BeautifulSoup pour analyser le texte HTML et extraire uniquement le texte brut sans balises.\n",
    "- Supprime les espaces superflus en début et fin de chaîne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \n",
    "    text = text.replace('\\xa0', ' ').strip()\n",
    "    # Remplace les caractères d'espace insécable ('\\xa0') par des espaces standard et supprime les espaces en début et fin de texte.\n",
    "    \n",
    "    text = text.replace('<li>', '<li>\\n')\n",
    "    # Ajoute un saut de ligne après chaque balise <li> pour une meilleure lisibilité du texte extrait.\n",
    "\n",
    "    soup = bs(text, 'html.parser')\n",
    "    # Utilise BeautifulSoup pour traiter le texte et supprimer toutes les balises HTML restantes.\n",
    "\n",
    "    cleaned_text = soup.get_text(separator='\\n').strip()\n",
    "    # Extrait uniquement le texte brut, en utilisant des sauts de ligne comme séparateurs entre les éléments,\n",
    "    # puis supprime les espaces en début et fin de texte.\n",
    "\n",
    "    return cleaned_text\n",
    "    # Retourne le texte nettoyé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation des informations extraites dans un fichier excel \n",
    "\n",
    "La fonction ``export_to_excel`` permet d'exporter une liste de données vers un fichier Excel structuré avec des en-têtes spécifiques. Elle crée un nouveau fichier Excel ou remplace l'ancien s'il existe déjà.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``data`` : Liste de listes contenant les données à exporter vers le fichier Excel. Chaque sous-liste représente une ligne dans la feuille Excel.\n",
    "\n",
    "- Retour : Aucun retour direct. Le fichier est enregistré dans le répertoire défini sous excel_folder.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Définit le nom du fichier Excel et les en-têtes des colonnes.\n",
    "- Supprime le fichier existant avec le même nom pour éviter les duplications de données.\n",
    "- Crée un nouveau classeur Excel, ajoute les en-têtes en première ligne, puis itère sur chaque ligne de data.\n",
    "- Applique la fonction clean_text pour nettoyer chaque élément de la ligne de données (à l’exception des colonnes \"Images\" et \"PDFs\") et ajoute les lignes dans la feuille Excel.\n",
    "- Sauvegarde le fichier et gère les erreurs en affichant un message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(data):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        excel_filename = os.path.join(excel_folder, 'data_lab_cercle.xlsx')\n",
    "        headers = ['Catégorie', 'date', 'Thème', 'Preface', 'Titre', 'Texte', 'Lien', 'Liens utiles']\n",
    "        # Nom du fichier Excel et définition des en-têtes des colonnes.\n",
    "\n",
    "        # Supprimer l'ancien fichier Excel s'il existe\n",
    "        if os.path.exists(excel_filename):\n",
    "            os.remove(excel_filename) \n",
    "        # Vérifie si un fichier Excel avec le même nom existe déjà et le supprime pour éviter la duplication.\n",
    "\n",
    "        wb = Workbook()\n",
    "        ws_general = wb.active\n",
    "        ws_general.title = 'Informations générales'\n",
    "        ws_general.append(headers)  # Ajouter les en-têtes\n",
    "        # Crée un nouveau classeur Excel et ajoute les en-têtes de colonnes dans la première ligne.\n",
    "\n",
    "        for line in data:\n",
    "            cleaned_line = [clean_text(item) if item else 'N/A' for item in line[:-2]]\n",
    "            # Nettoie chaque élément de la ligne de données avec `clean_text`, sauf les colonnes \"Images\" et \"PDFs\".\n",
    "            ws_general.append(cleaned_line)\n",
    "            # Ajoute la ligne nettoyée dans la feuille Excel.\n",
    "\n",
    "        wb.save(excel_filename)\n",
    "        print(f\"Données exportées avec succès vers : {excel_filename}\")\n",
    "        # Sauvegarde le classeur Excel sous le nom spécifié.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de l'exportation vers Excel : {str(e)}\")\n",
    "        # En cas d'erreur, affiche un message indiquant la nature de l'erreur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Télechargement des images\n",
    "\n",
    "La fonction ``download_images`` télécharge toutes les images disponibles dans une page web et les enregistre localement dans un dossier défini pour les images. Elle utilise BeautifulSoup pour trouver les balises d'image et requests pour récupérer le contenu des images.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``soup`` : Un objet BeautifulSoup représentant la page web analysée.\n",
    "- ``link`` : L'URL de la page d'où proviennent les images, utilisée pour générer des noms uniques pour les fichiers.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie une liste contenant les chemins des images téléchargées. Si aucune image n'est téléchargée ou en cas d'erreur, retourne ``['N/A']``.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Récupère toutes les balises`` <img>`` avec un attribut src pour collecter les URLs des images.\n",
    "- Pour chaque URL d'image, télécharge l'image en utilisant ``requests.get``.\n",
    "- Génére un nom unique pour chaque image en utilisant une partie de l'URL de la page et un index.\n",
    "- Sauvegarde chaque image dans le dossier défini, puis ajoute le chemin du fichier téléchargé à une liste.\n",
    "- Retourne la liste des chemins des images téléchargées ou ``['N/A']`` en cas d'échec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(soup, link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        images = soup.find_all('img')\n",
    "        image_urls = [img['src'] for img in images if 'src' in img.attrs]\n",
    "        # Récupère toutes les balises <img> avec un attribut 'src' et compile leurs URLs.\n",
    "\n",
    "        downloaded_images = []\n",
    "        # Initialise une liste pour stocker les chemins des images téléchargées.\n",
    "\n",
    "        for i, img_url in enumerate(image_urls):\n",
    "            img_data = requests.get(img_url).content\n",
    "            # Télécharge le contenu de l'image depuis son URL.\n",
    "\n",
    "            img_name = f\"{link.split('/')[-1]}_{i}.jpg\"\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            # Définit un nom unique pour chaque image en utilisant une partie de l'URL de la page source et un index.\n",
    "            \n",
    "            with open(img_path, 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "                # Écrit le contenu de l'image dans un fichier local avec accès en mode écriture binaire.\n",
    "            \n",
    "            downloaded_images.append(img_path)\n",
    "            # Ajoute le chemin de l'image téléchargée à la liste.\n",
    "\n",
    "        return downloaded_images if downloaded_images else ['N/A']\n",
    "        # Retourne la liste des images téléchargées, ou 'N/A' si aucune image n'a été téléchargée.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors du téléchargement des images : {str(e)}\")\n",
    "        return ['N/A']\n",
    "        # En cas d'erreur, affiche un message d'erreur et retourne 'N/A'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement des pdfs\n",
    "\n",
    "La fonction ``download_pdfs`` télécharge tous les fichiers PDF présents sur une page web, les sauvegarde localement, puis renvoie les chemins des fichiers enregistrés.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- ``soup`` : Un objet BeautifulSoup représentant le contenu HTML de la page web.\n",
    "- ``link`` : L'URL de la page d'où proviennent les fichiers PDF, utilisée pour créer des noms de fichiers uniques.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie une liste contenant les chemins des fichiers PDF téléchargés. Si aucun PDF n'est téléchargé ou en cas d'erreur, retourne ``['N/A']``.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Trouve toutes les balises ``<div>`` qui contiennent des liens vers des fichiers PDF en ciblant les classes CSS spécifiques.\n",
    "- Extrait les URLs des fichiers PDF en vérifiant que le lien contient le titre 'Télécharger'.\n",
    "- Pour chaque URL, télécharge le fichier PDF en utilisant requests.get.\n",
    "- Génère un nom unique pour chaque PDF en utilisant une partie de l’URL de la page et un index.\n",
    "- Sauvegarde chaque PDF dans le dossier défini et ajoute le chemin du fichier téléchargé à une liste.\n",
    "- Retourne la liste des chemins des PDF téléchargés ou ``['N/A']`` en cas d'absence de téléchargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(soup, link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        pdf_divs = soup.find_all('div', class_=[\n",
    "            'col-sm-12 col-xl-5- col-xxl-4 action-hover- d-flex flex-column justify-center', \n",
    "            'col-sm-3 col-md-6- col-lg-4 col-xl-3 col-xxl-2 action-hover- d-flex flex-column justify-center align-items-center'\n",
    "        ])\n",
    "        # Recherche toutes les balises <div> contenant des PDFs dans des sections spécifiques en fonction de leurs classes.\n",
    "\n",
    "        pdf_urls = [\n",
    "            div.find('a', href=True)['href']\n",
    "            for div in pdf_divs\n",
    "            if div.find('a', href=True) and div.find('a', href=True).get('title') == 'Télécharger'\n",
    "        ]\n",
    "        # Récupère les URLs des PDF uniquement dans les balises <a> ayant le titre 'Télécharger'.\n",
    "\n",
    "        downloaded_pdfs = []\n",
    "        # Initialise une liste pour stocker les chemins des PDFs téléchargés.\n",
    "\n",
    "        for i, pdf_url in enumerate(pdf_urls):\n",
    "            pdf_data = requests.get(pdf_url).content\n",
    "            # Télécharge le contenu du PDF depuis son URL.\n",
    "\n",
    "            pdf_name = f\"{link.split('/')[-1]}_{i}.pdf\"\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_name)\n",
    "            # Définit un nom unique pour chaque PDF en utilisant une partie de l'URL de la page source et un index.\n",
    "\n",
    "            with open(pdf_path, 'wb') as handler:\n",
    "                handler.write(pdf_data)\n",
    "                # Écrit le contenu du PDF dans un fichier local avec accès en mode écriture binaire.\n",
    "\n",
    "            downloaded_pdfs.append(pdf_path)\n",
    "            # Ajoute le chemin du PDF téléchargé à la liste.\n",
    "\n",
    "        return downloaded_pdfs if downloaded_pdfs else ['N/A']\n",
    "        # Retourne la liste des PDFs téléchargés, ou 'N/A' si aucun PDF n'a été téléchargé.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors du téléchargement des PDFs : {str(e)}\")\n",
    "        return ['N/A']\n",
    "        # En cas d'erreur, affiche un message d'erreur et retourne 'N/A'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des informations depuis une page web et téléchargement des pdfs et images associés\n",
    "\n",
    "La fonction ``get_info extrait`` des informations spécifiques (catégorie, thème, préface, date, titre, texte, liens utiles) depuis une page web. Elle télécharge également les images et fichiers PDF associés, puis retourne les informations sous forme de liste structurée.\n",
    "\n",
    "Paramètres :\n",
    "\n",
    "- `link` : URL de la page web d'où les informations doivent être extraites.\n",
    "\n",
    "Retour :\n",
    "\n",
    "- Renvoie une liste contenant les informations extraites et les chemins des images et PDFs téléchargés. En cas d'erreur, retourne une liste avec des valeurs N/A.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Envoie une requête pour récupérer le contenu HTML de la page, puis utilise BeautifulSoup pour analyser ce contenu.\n",
    "- Extrait la catégorie, le thème, la préface, la date, les titres et le texte de la page. Pour chaque élément, vérifie si une valeur est disponible ; sinon, utilise N/A.\n",
    "- Recherche des liens utiles dans les balises`` <a>`` ayant pour titre ``\"Je me lance\"`` ou ``\"Accéder à l'outil\"`` et les concatène dans une chaîne.\n",
    "- Appelle download_images et download_pdfs pour télécharger les images et les fichiers PDF présents sur la page, en sauvegardant les chemins d’accès.\n",
    "- Retourne une liste contenant toutes les informations extraites et les chemins des fichiers téléchargés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(link):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        page = requests.get(link)\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        # Envoie une requête HTTP pour obtenir le contenu de la page et utilise BeautifulSoup pour l'analyser.\n",
    "\n",
    "        # Vérification et extraction des informations avec gestion du 'N/A'\n",
    "        categorie_element = soup.find('h1', class_='h2')\n",
    "        categorie = categorie_element.text.strip() if categorie_element else 'N/A'\n",
    "        # Extrait la catégorie, ou 'N/A' si elle est absente.\n",
    "\n",
    "        theme_element = soup.find('h1', class_='wiki-title-theme')\n",
    "        theme = theme_element.text.strip() if theme_element else 'N/A'\n",
    "        # Extrait le thème de la page.\n",
    "\n",
    "        preface_element = soup.find('p', class_='lead')\n",
    "        preface = preface_element.text.strip() if preface_element else 'N/A'\n",
    "        # Extrait le texte de préface.\n",
    "\n",
    "        date_element = soup.find('small', class_='app-card-date text-muted')\n",
    "        date = date_element.text.strip() if date_element else 'N/A'\n",
    "        # Extrait la date de la page.\n",
    "\n",
    "        titre_elements = soup.find_all('h2', class_=['wiki-title-def', 'app-post-title'])\n",
    "        titre = ' / '.join([elem.text.strip() for elem in titre_elements]) if titre_elements else 'N/A'\n",
    "        # Concatène tous les titres de la page en les séparant par ' / '.\n",
    "\n",
    "        texte_elements = soup.find_all(['div', 'article'], class_=['card-text', 'kPost-content'])\n",
    "        texte = ' '.join([elem.get_text(separator='\\n').strip() if elem else 'N/A' for elem in texte_elements]) if texte_elements else 'N/A'\n",
    "        # Concatène tout le texte principal de la page avec des sauts de ligne.\n",
    "\n",
    "        # Récupérer l'élément <a> spécifique avec title \"Je me lance\" ou \"Accéder à l'outil\"\n",
    "        liens_utiles = []  # Liste pour stocker tous les liens trouvés\n",
    "        for a_tag in soup.find_all('a', href=True, title=True):\n",
    "            if a_tag['title'].strip() in [\"Je me lance\", \"Accéder à l'outil\"]:\n",
    "                liens_utiles.append(a_tag['href'])\n",
    "        # Ajoute les URLs des liens utiles si le titre correspond à \"Je me lance\" ou \"Accéder à l'outil\".\n",
    "\n",
    "        # Si des liens ont été trouvés, les joindre en chaîne, sinon mettre \"N/A\"\n",
    "        liens_utiles = ', '.join(liens_utiles) if liens_utiles else 'N/A'\n",
    "        # Convertit la liste de liens utiles en une chaîne de texte, ou 'N/A' si aucun lien trouvé.\n",
    "\n",
    "        downloaded_images = download_images(soup, link)\n",
    "        image_paths = ', '.join(downloaded_images) if downloaded_images else 'N/A'\n",
    "        # Télécharge les images de la page et les convertit en une chaîne de chemins ou 'N/A'.\n",
    "\n",
    "        downloaded_pdfs = download_pdfs(soup, link)\n",
    "        pdf_paths = ', '.join(downloaded_pdfs) if downloaded_pdfs else 'N/A'\n",
    "        # Télécharge les PDFs de la page et les convertit en une chaîne de chemins ou 'N/A'.\n",
    "\n",
    "        # Retourner les informations extraites sous forme de liste\n",
    "        line = [\n",
    "            categorie,\n",
    "            date,\n",
    "            theme,\n",
    "            preface,\n",
    "            titre,\n",
    "            texte,\n",
    "            link,\n",
    "            liens_utiles,  # Insertion de liens utiles ici\n",
    "            image_paths,\n",
    "            pdf_paths\n",
    "        ]\n",
    "        # Structure les informations dans une liste avec l’ordre des éléments requis.\n",
    "\n",
    "        return line\n",
    "        # Retourne la liste contenant toutes les informations extraites.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de l'extraction des informations : {str(e)}\")\n",
    "        return ['N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', link, 'N/A', 'N/A', 'N/A']\n",
    "        # En cas d'erreur, retourne une liste de valeurs 'N/A' pour chaque élément sauf le lien d'origine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des liens d'une section spécifique d'une page web\n",
    "\n",
    "Ce script extrait des liens de sections spécifiques d'une page web, récupère des informations de chaque lien, puis exporte toutes les données dans un fichier Excel.\n",
    "\n",
    "Fonction e``xtract_links(section_id)`` :\n",
    "\n",
    "- Paramètre : ``section_id –`` l'ID de la section contenant les liens à extraire.\n",
    "- Retour : Une liste contenant les URLs de chaque lien trouvé dans la section.\n",
    "\n",
    "Processus :\n",
    "\n",
    "- Utilise BeautifulSoup pour trouver une ``<div>`` avec l’ID spécifié.\n",
    "- Extrait tous les liens (href) des éléments ``<li>`` de cette section et les stocke dans une liste.\n",
    "\n",
    "- Bloc ``if __name__ == \"__main__\":`` :\n",
    "\n",
    "- Définit l’URL cible et analyse le contenu de la page avec BeautifulSoup.\n",
    "- Appelle extract_links pour extraire les liens des sections identifiées par c``ollapse-bas-carbone`` et c``ollapse-bim``.\n",
    "- Affiche les liens extraits pour vérification.\n",
    "- Combine tous les liens en une liste unique ``all_links``.\n",
    "- Pour chaque lien, appelle get_info pour extraire les informations détaillées et les stocke dans ``all_data``.\n",
    "- Exporte les données collectées dans un fichier Excel avec `export_to_excel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(section_id):\n",
    "    \n",
    "    section = soup.find('div', id=section_id)\n",
    "    links = []\n",
    "    # Initialise une liste vide pour stocker les liens extraits.\n",
    "\n",
    "    if section:\n",
    "        list_items = section.find_all('li', class_='list-item col-12 col-lg-6 py-2')\n",
    "        # Recherche tous les éléments <li> dans la section spécifiée, qui contiennent les liens recherchés.\n",
    "        \n",
    "        for item in list_items:\n",
    "            link = item.find('a', href=True)\n",
    "            if link:\n",
    "                links.append(link['href'])\n",
    "                # Ajoute l'attribut 'href' du lien trouvé à la liste des liens si le lien existe.\n",
    "    \n",
    "    return links\n",
    "    # Retourne la liste de tous les liens extraits de la section.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Point d'entrée principal du programme pour extraire les liens, obtenir des informations et les exporter dans un fichier Excel.\n",
    "    \n",
    "    url = \"https://lab.cercle-promodul.inef4.org/themes\"\n",
    "    # URL de la page cible pour récupérer les sections spécifiques de liens.\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "    # Envoie une requête pour obtenir le contenu de la page et utilise BeautifulSoup pour analyser le HTML.\n",
    "\n",
    "    links_bas_carbone = extract_links('collapse-bas-carbone')\n",
    "    links_bim = extract_links('collapse-bim')\n",
    "    # Appelle la fonction extract_links pour obtenir les liens des sections \"Bas carbone\" et \"BIM\".\n",
    "\n",
    "    print(\"Liens de la section 'Bas carbone':\")\n",
    "    for link in links_bas_carbone:\n",
    "        print(link)\n",
    "        # Affiche tous les liens trouvés dans la section \"Bas carbone\".\n",
    "\n",
    "    print(\"\\nLiens de la section 'BIM':\")\n",
    "    for link in links_bim:\n",
    "        print(link)\n",
    "        # Affiche tous les liens trouvés dans la section \"BIM\".\n",
    "\n",
    "    all_links = links_bas_carbone + links_bim\n",
    "    # Combine tous les liens des sections \"Bas carbone\" et \"BIM\" en une seule liste.\n",
    "\n",
    "    all_data = []\n",
    "    # Initialise une liste pour stocker toutes les informations extraites des liens.\n",
    "\n",
    "    for link in all_links:\n",
    "        info = get_info(link)\n",
    "        all_data.append(info)\n",
    "        # Pour chaque lien, extrait les informations et les ajoute à la liste de données.\n",
    "\n",
    "    export_to_excel(all_data)\n",
    "    # Exporte toutes les informations extraites dans un fichier Excel en appelant la fonction export_to_excel.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
